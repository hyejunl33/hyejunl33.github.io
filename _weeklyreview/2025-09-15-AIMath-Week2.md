---
title: "Week2_AIMath_학습회고"
date: 2025-09-16 20:00:00 +0900
tags:
  - 선형대수
  - 확률론
  - 통계학
  - 경사하강법
excerpt: "벡터, 행렬, 텐서의 개념부터 확률분포, 최대가능도 추정법(MLE), 그리고 경사하강법을 이용한 모델 최적화까지 2주차 핵심 내용을 정리합니다."
math: true
---

## 목차
1.  [선형대수: 벡터, 행렬, 텐서](#1-선형대수-벡터-행렬-텐서)
    -   [벡터의 정의와 연산](#벡터의-정의와-연산)
    -   [벡터의 노름(Norm)과 내적(Inner Product)](#벡터의-노름norm과-내적inner-product)
    -   [행렬의 정의와 연산](#행렬의-정의와-연산)
    -   [역행렬과 행렬식](#역행렬과-행렬식)
    -   [고유값 분해와 특이값 분해(SVD)](#고유값-분해와-특이값-분해svd)
    -   [텐서(Tensor)](#텐서tensor)
2.  [확률론과 통계학](#2-확률론과-통계학)
    -   [확률분포의 이해](#확률분포의-이해)
    -   [조건부 확률과 베이즈 정리](#조건부-확률과-베이즈-정리)
    -   [최대가능도 추정법 (MLE)](#최대가능도-추정법-mle)
    -   [딥러닝과 통계학의 연결점](#딥러닝과-통계학의-연결점)
3.  [경사하강법](#3-경사하강법)
    -   [미분과 경사하강법의 원리](#미분과-경사하강법의-원리)
    -   [확률적 경사하강법 (SGD)](#확률적-경사하강법-sgd)

---

## 1. 선형대수: 벡터, 행렬, 텐서

### 벡터의 정의와 연산
- **벡터**는 숫자를 원소로 가지는 배열로 정의하며, 공간에서의 한 점 또는 원점으로부터의 상대적 위치로 표현한다.
- 벡터의 **스칼라곱**은 벡터의 방향은 유지한 채 길이만을 변경하는 연산이다. 곱하는 스칼라 값이 음수일 경우 방향이 반대가 된다.
- 차원이 같은 두 벡터는 **덧셈, 뺄셈, 성분곱(Hadamard product)**이 가능하다. 벡터의 덧셈은 한 벡터의 끝에서 다른 벡터의 상대적 위치 이동으로 시각화할 수 있다.

### 벡터의 노름(Norm)과 내적(Inner Product)
- **노름(Norm)**은 원점으로부터 벡터까지의 거리를 의미하며, 벡터의 크기를 나타낸다.
- **L1 노름**은 각 성분의 절댓값의 합으로 계산된다: \\(||x||_{1}=\sum_{i=1}^{d}|x_{i}|\\).
- **L2 노름**은 유클리드 거리로, 각 성분의 제곱 합의 제곱근으로 계산된다: \\(||x||_{2}=\sqrt{\sum_{i=1}^{d}|x_{i}|^{2}}\\).
- **내적(Inner product)**은 두 벡터의 유사도를 측정하는 데 사용되며, 정사영(orthogonal projection)된 벡터의 길이와 관련이 있다. 내적은 제2 코사인 법칙을 통해 두 벡터 사이의 각도를 계산하는 데 활용한다.
    $$\langle x, y \rangle = ||x||_2 ||y||_2 \cos\theta$$

### 행렬의 정의와 연산
- **행렬**은 벡터를 원소로 가지는 2차원 배열로, 공간상의 여러 점들을 나타낸다.
- **전치행렬(Transpose matrix)**은 원래 행렬의 행과 열 인덱스를 바꾼 행렬을 의미한다: \\(X^{\top}=(x_{ij})^{\top}=(x_{ji})\\).
- **행렬 곱셈**은 \\(i\\)번째 행벡터와 \\(j\\)번째 열벡터 사이의 내적을 성분으로 가지는 새로운 행렬을 계산하는 연산이다. 행렬 곱셈이 성립하려면 앞 행렬의 열의 개수와 뒤 행렬의 행의 개수가 같아야 한다.
    $$(XY)_{ij} = \sum_{k}X_{ik}Y_{kj}$$

### 역행렬과 행렬식
- **역행렬(Inverse matrix)**은 어떤 행렬 A의 연산을 거꾸로 되돌리는 행렬로, \\(A^{-1}\\)로 표기한다. 행렬과 그 역행렬의 곱은 항등행렬(\\(I\\))이 된다: \\(AA^{-1}=A^{-1}A=I\\).
- 역행렬은 행과 열의 수가 같은 **정사각행렬**이면서 **행렬식(determinant)이 0이 아닐 때**만 존재한다.
- **행렬식**의 절댓값은 행렬을 구성하는 벡터들이 만드는 다포체(polytope)의 부피(2차원에서는 넓이)와 같다. 행렬을 구성하는 벡터들이 선형 종속(평행하거나, 다른 벡터의 선형 결합으로 표현 가능)일 경우 다포체를 만들 수 없으므로 행렬식은 0이 되고, 역행렬은 존재하지 않는다.

### 고유값 분해와 특이값 분해(SVD)
- **고유값(Eigenvalue)**과 **고유벡터(Eigenvector)**는 정사각행렬 \\(A\\)에 대해 다음 식을 만족하는 스칼라 \\(\lambda\\)와 벡터 \\(v\\)를 의미한다.
    $$Av = \lambda v$$
    이는 고유벡터 \\(v\\)에 행렬 \\(A\\)를 곱해 선형 변환을 해도 방향은 변하지 않고 크기만 \\(\lambda\\)배만큼 변한다는 것을 의미한다.
- **고유값 분해(Eigenvalue Decomposition)**는 행렬 \\(A\\)를 고유벡터로 이루어진 행렬 \\(V\\)와 고유값으로 이루어진 대각 행렬 \\(\Lambda\\)의 곱으로 분해하는 것이다: \\(A = V\Lambda V^{-1}\\). 이는 데이터의 공분산 행렬에서 주성분(Principal Component)을 찾는 **주성분 분석(PCA)**의 핵심 원리이다.
- **특이값 분해(Singular Value Decomposition, SVD)**는 정사각행렬이 아닌 일반적인 행렬에 대해서도 적용할 수 있는 분해 방법이다.
    $$A_{n\times m} = U_{n\times n}\Sigma_{n\times m}V^{\top}_{m\times m}$$
    SVD는 **유사역행렬(Moore-Penrose pseudo-inverse)**을 계산하는 데 사용되어, 역행렬이 존재하지 않는 경우에도 선형회귀 문제의 해를 근사적으로 구할 수 있게 해준다.

### 텐서(Tensor)
- N차원 **텐서**는 N-1차원 텐서를 원소로 가지는 다차원 배열이다. 벡터는 1차원 텐서, 행렬은 2차원 텐서로 볼 수 있다.
- 컬러 영상 데이터는 (채널, 높이, 너비)의 3차원 텐서로, 여러 영상 데이터의 묶음(미니배치)은 (배치 크기, 채널, 높이, 너비)의 4차원 텐서로 표현된다.

---

## 2. 확률론과 통계학

### 확률분포의 이해
- **통계적 모델링**은 적절한 가정 위에서 데이터의 확률분포를 추정하는 기법이다.
- **모수적(parametric) 방법론**은 데이터가 특정 확률분포(정규분포, 베르누이 분포 등)를 따른다고 가정한 후, 그 분포의 모수(parameter)를 추정하는 방식이다.
- **비모수적(nonparametric) 방법론**은 특정 분포를 가정하지 않고, 데이터에 따라 모델의 구조와 모수의 개수가 유연하게 바뀌는 방식이다.
- **결합분포 \\(P(x,y)\\)**는 입력 \\(x\\)와 출력 \\(y\\)가 함께 나타날 확률로, 데이터 전체의 분포 \\(P_{data}\\)를 모델링한다.
- **주변분포 \\(P(x)\\)**는 \\(y\\)에 대한 정보를 없애고 \\(x\\)에 대한 확률분포만 남긴 것으로, 결합분포를 \\(y\\)에 대해 합하거나 적분하여 구한다: \\(P(x) = \sum_{y}P(x,y)\\).

### 조건부 확률과 베이즈 정리
- **조건부 확률 \\(P(x|y)\\)**는 특정 클래스 \\(y\\)가 주어졌을 때 데이터 \\(x\\)의 분포를 나타내며, 입력과 출력 사이의 관계를 모델링한다.
- **베이즈 정리**는 조건부 확률을 이용해 새로운 정보가 주어졌을 때 기존의 확률을 업데이트하는 방법을 제공한다.
    $$P(\theta|\mathcal{D}) = P(\theta)\frac{P(\mathcal{D}|\theta)}{P(\mathcal{D})}$$
    - \\(P(\theta)\\) : **사전 확률(Prior)** - 정보를 얻기 전의 확률.
    - \\(P(\mathcal{D}|\theta)\\) : **가능도(Likelihood)** - 가설 \\(\theta\\)가 주어졌을 때 데이터 \\(\mathcal{D}\\)가 관찰될 확률.
    - \\(P(\mathcal{D})\\) : **증거(Evidence)** - 데이터 \\(\mathcal{D}\\)의 주변 확률.
    - \\(P(\theta|\mathcal{D})\\) : **사후 확률(Posterior)** - 데이터를 관찰한 후 업데이트된 확률.

### 최대가능도 추정법 (MLE)
- **최대가능도 추정법(MLE)**은 관측된 데이터 집합이 나타날 가능성을 최대로 만드는 모수 \\(\theta\\)를 찾는 방법이다.
    $$\hat{\theta}_{MLE} = \underset{\theta}{\mathrm{argmax}} L(\theta;x) = \underset{\theta}{\mathrm{argmax}} P(x|\theta)$$
- 데이터가 독립적으로 추출되었다고 가정하면, 가능도는 각 데이터 포인트의 확률의 곱으로 나타난다. 로그 함수는 단조 증가 함수이므로, 가능도를 직접 최대화하는 대신 **로그가능도**를 최대화해도 동일한 최적해를 얻을 수 있다. 이는 곱셈을 덧셈으로 바꿔주어 연산을 간편하게 하고 컴퓨터의 언더플로우 문제를 방지한다.
    $$\log L(\theta;X) = \sum_{i=1}^{n}\log P(x_{i}|\theta)$$
- **정규분포**의 모수를 MLE로 추정하면, 그 결과는 표본평균과 표본분산이 된다.
- **카테고리 분포**의 모수를 MLE로 추정할 때는 라그랑주 승수법을 이용하여 제약식(\\(\sum p_k = 1\\))을 만족시키며, 그 결과는 각 카테고리가 관측된 횟수의 비율이 된다.

### 딥러닝과 통계학의 연결점
- 기계학습의 **손실 함수(loss function)**들은 통계학적 원리로부터 유도된다.
- **평균제곱오차(MSE)**는 예측 오차의 분산을 최소화하는 방향으로 학습을 유도한다.
    $$l(f(x_i), y_i) = ||f(x_i) - y_i||^2$$
- **교차 엔트로피(Cross-Entropy)**는 분류 문제에서 모델 예측의 로그가능도를 최대화하는 것과 동일하며, 이는 정답 분포 P와 모델 예측 분포 Q 사이의 **쿨백-라이블러 발산(KL Divergence)**을 최소화하는 것과 같다.
    $$KL(P||Q) = -\mathbb{E}_{X \sim P(x)}[\log Q(x)] + \mathbb{E}_{X \sim P(x)}[\log P(x)]$$

---

## 3. 경사하강법

### 미분과 경사하강법의 원리
- **미분**은 변수의 움직임에 따른 함수 값의 변화율, 즉 특정 지점에서의 접선의 기울기를 측정하는 도구이다.
- 함수의 특정 지점에서 미분값을 알면, 함수 값을 증가시키거나 감소시키기 위해 변수를 어느 방향으로 움직여야 할지 알 수 있다. 함수 값을 감소시키려면 미분값을 빼야 한다.
- **그래디언트(Gradient) 벡터 \\(\nabla f\\)**는 각 변수에 대한 편미분을 원소로 가지는 벡터로, 함수 값이 가장 빠르게 증가하는 방향을 가리킨다.
- **경사하강법**은 손실 함수의 값을 최소화하기 위해, 현재 파라미터 위치에서 계산된 그래디언트의 반대 방향으로 파라미터를 반복적으로 업데이트하는 최적화 알고리즘이다.

```python
'''경사하강법 알고리즘 의사코드'''
    var = init
    grad = gradient(var)
    while (norm(grad) > eps):
        var = var - lr * grad
        grad = gradient(var)
```
- 선형 회귀의 목적식 \\(L = ||y-X\beta||_2^2\\) 을 최소화하기 위한 그래디언트는 다음과 같이 유도된다.
    $$\nabla_{\beta}||y-X\beta||_{2}^{2} = -\frac{2}{n}X^{\top}(y-X\beta)$$

### 확률적 경사하강법 (SGD)
- **경사하강법**은 그래디언트를 계산할 때 전체 데이터를 모두 사용하는 반면, **확률적 경사하강법(SGD)**은 데이터의 일부, 즉 **미니배치(mini-batch)**를 무작위로 추출하여 그래디언트를 근사적으로 계산한다.
- SGD는 전체 데이터를 사용하는 것보다 연산량이 훨씬 적어 대규모 데이터셋에 효율적이다.
- 미니배치를 무작위로 선택하기 때문에 목적 함수의 모양이 매 스텝마다 조금씩 바뀌게 되어, 결과가 랜덤성을 가진다. 이러한 노이즈(noise)는 오히려 non-convex 함수에서 **지역 최솟값(local minima)을 탈출**하는 데 도움을 줄 수 있어, 딥러닝 모델 최적화에 더 효과적이다.

## 2. 피어세션 정리

지게차와 크레인, 석유시추, 퍼즐게임, 충돌위험 찾기, 택배배달과 수거하기 문제를 풀었다. 대부분 lv2 중상정도 난이도라 무탈하게 푼것같다.

그리고 금요일에 Attention is All you need라는 논문을 주제로 발표를 진행했다. 블로그에 내용정리한 글을 따로 업로드할 예정이다.

## 3. 소감

네이버블로그는 마크다운을 지원하지 않아서 수식하나 작성하는데 너무 오래걸리고 힘들다.. inline함수도 노션은 컨트롤e누르면 바로 바뀌는데, 네이버블로그는 보통불편한게 아닌거같다.. velog나 티스토리로 옮겨야겠다.

저번주보단 나름 익숙해진 한주였다. 과제도 미리미리 했고, 강의도 밀리지 않고 들었다. 그런데, SVD, PCA, SGD를 이용한 선형회귀에서 라그랑주승수법이나, 고유벡터, Rank개념같이 모르는 개념이 많이나와서 이해하기 어려웠다. 원서를 찾아보면서 이해해보록 해야겠다.

Attention is all you need논문은 처음에는 막막했는데, 막상 이해해보려고 하니깐 여기저기 찾아볼수록 겹치는 내용들이 많아서 어느정도 이해한것 같다. Attention을 기반으로한 ViT논문도 읽어보면서 Diffusion을 기반으로한 이미지 generation model도 이해해보고 싶다.