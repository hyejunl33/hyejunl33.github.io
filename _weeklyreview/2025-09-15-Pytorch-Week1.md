---
title: "Week1_Pytorch_학습회고"
date: 2025-09-15
tags:
  - PyTorch
  - Deep-Learning
---


# 1. 강의 복습

## PyTorch와 Tensor
◦ PyTorch는 딥러닝 모델 구현과 실행을 위한 프로그래밍 인터페이스다.  
◦ Tensor는 PyTorch의 핵심 데이터 구조로, 스칼라(0차원)부터 다차원 배열까지 데이터를 표현한다.  
◦ Tensor는 다양한 데이터 타입(정수형, 실수형 등)을 가지며, 타입 캐스팅으로 변환할 수 있다.  
◦ Tensor의 기초 함수로는 최소/최대값, 합계, 평균 등을 계산하며, 메서드로 차원 수, 크기, 요소 개수 등을 확인할 수 있다.  

## Tensor 연산 및 조작
◦ **Tensor 생성**: 특정 값, 난수, 리스트/Numpy 데이터 등으로 Tensor를 만들 수 있다.  
◦ **Tensor 모양 변경**: `view()`, `flatten()`, `reshape()`, `transpose()`, `squeeze()`, `unsqueeze()`, `stack()`, `cat()` 함수 등으로 Tensor의 모양이나 차원을 변경하고 연결할 수 있다.  
◦ **Tensor 연산**: 요소별 산술 연산, 비교 연산, 논리 연산을 지원한다.  
◦ **노름(Norm)**: 1-D Tensor(Vector)가 원점에서 얼마나 떨어져 있는지 나타내는 값으로, L1, L2, L∞ 노름 등이 존재한다.  
◦ **유사도(Similarity)**: 두 1-D Tensor(Vector)가 얼마나 유사한지 측정하는 값으로, 맨해튼, 유클리드, 코사인 유사도 등이 있다.  
◦ **행렬 곱셈**: 2차원 Tensor(Matrix) 간의 곱셈으로, 두 행렬을 결합하여 새로운 행렬을 생성하는 연산이며, 신경망 구현의 핵심 연산이다.  
◦ **CUDA Tensor**: GPU를 사용하여 Tensor 연산을 수행하여 속도 및 효율성을 높일 수 있음.  

## 선형 회귀 모델(Linear Regression)
◦ 선형 회귀는 주어진 데이터를 통해 특징 변수와 목표 변수 사이의 선형 관계를 분석하고, 이를 바탕으로 모델을 학습시켜 새로운 데이터의 결과를 연속적인 숫자 값으로 예측하는 과정이다.  
◦ 상관 관계 분석을 통해 두 변수 간의 선형 관계를 파악한다.  
◦ 학습은 `y = wx + b` 식에서 최적의 가중치(w)와 바이어스(b)를 찾는 과정이다.  
◦ `nn.Module`을 상속받아 PyTorch에서 선형 회귀 모델을 구축한다.  
◦ 손실 함수는 목표 변수와 예측 변수 간의 차이를 측정하며, 모델을 최적화하기 위해 사용한다. 선형 회귀에서는 주로 평균 제곱 오차(MSE)를 사용한다.  
◦ 경사하강법은 머신러닝의 최적화 알고리즘 중 하나로, 주어진 손실 함수에서 모델의 가중치와 바이어스의 최적의 값을 찾기 위해 사용한다. 학습률은 모델이 학습할 때 가중치가 업데이트되는 크기를 결정한다.  
◦ 확률적 경사하강법(SGD)은 각 데이터 포인트마다 오차를 계산하여 가중치와 바이어스를 업데이트하는 최적화 알고리즘으로, 대규모 데이터셋의 계산 비용 및 로컬 미니마 문제를 보완할 수 있다.  
◦ 에폭은 모델이 전체 데이터셋을 한 번 완전히 학습하는 과정을 의미하며, 너무 많으면 과적합이 발생할 수 있다.  
◦ 데이터 표준화는 특징 변수와 목표 변수의 평균을 0, 분산을 1로 맞추어 손실 값 문제를 해결하고 학습 효율을 높이는 방법이다.  

## 이진 분류 모델(Binary Classification)
◦ 이진 분류는 주어진 데이터를 학습하여 새로운 데이터를 사전에 정의된 두 가지 범주 중 하나로 분류하는 예측 모델을 구축하는 과정이다. (예: 붓꽃 분류, 스팸 메일 분류, 금융 사기 탐지, 의료 진단).  
◦ 로지스틱 회귀는 이진 분류의 대표적인 알고리즘으로, 트레이닝 데이터의 특성과 분포를 바탕으로 최적의 결정 경계를 찾아 시그모이드 함수를 통해 데이터를 이진 분류한다.  
◦ 이진 교차 엔트로피(BCE)는 이진 분류 문제에서 모델의 예측 변수와 목표 변수 간의 차이를 측정하기 위해 사용되는 손실 함수이다.  
◦ `Dataset` & `DataLoader`는 PyTorch에서 데이터의 전처리 및 배치 처리를 용이하게 할 수 있도록 사용하는 클래스다.  
◦ 미니배치 경사하강법은 경사하강법과 확률적 경사하강법의 장단점을 보완한 알고리즘이다.  
◦ 모델 테스트는 트레이닝 데이터에 포함되지 않은 새로운 데이터를 사전에 정의된 두 가지 범주 중 하나로 분류하는 모델을 구축하는 과정이다. 테스트 데이터를 입력받아 예측된 결과를 반환하고, 예측값과 실제 라벨을 출력 및 시각화하여 성능을 평가한다.  

# 2. 피어세션 정리
1주차 피어세션때는 오전시간에 푼 프로그래머스 문제 코드를 서로에게 설명해주는 시간을 가졌다. 문제풀이 시간 후에 `itertools` 라이브러리에 있는 `Combinations` 함수나, DFS/BFS 알고리즘, `Collections`에 있는 각종 dict 등 python에서 알고리즘 문제풀이에 사용하는 알고리즘을 슬랙 캔버스에 함께 정리하는 시간을 가졌다.

여름방학 내내 코테준비하며 알고리즘공부를 나름 열심히 해왔지만, 부캠 코테를 붙고나서 알고리즘 공부를 아예 안하다가 문제를 풀려니깐, 벽이 느껴졌던것 같다. 이번주까지만 알고리즘공부를 피어세션시간에 하고 다음주부터는 논문 하나씩을 발표할 예정인데, 잘할 수 있을까..?

> *Attention is all you need*

NLP 관련 논문이긴 하지만, 프리코스를 들을때, Attention을 이용한 Transformer의 전후로 번역기가 달라진다는 얘기를 듣고 궁금해진 논문이다. 프리코스를 들으며 전반적인 내용을 이해하고 있긴 하지만, Code를 뜯어보며 이해해보진 않아서, 다음주, 다다음주 피어세션때 논문발표를 준비하며 코드를 뜯어보며 이해해볼 예정이다.  

# 3. 과제수행/과제 결과물에 대한 정리
과제1은 전반적으로 PyTorch를 조작하고, 생성하는 기본적인 과제였고, 과제 2, 3은 `nn.Module`을 이용해서 학습하는 코드를 작성하는 과제였다. 프로그래머스 레벨0문제를 풀때 빈칸에 코드를 넣는식으로 푸는 문제랑 비슷한 느낌인데, 내가 잘 모르는 PyTorch 툴들을 이용해야되니까 낯설긴 했다.

알고리즘문제를 풀때는 기껏해야 2차원 리스트정도만 다뤘는데, 텐서로 데이터를 다룰려니까, `View()`나 `reshape()`, `flatten()`같이 차원을 바꿔줘야 되는 경우도 생기고, `Linear(a,b)` 함수처럼 a차원으로 입력을 받고 b차원으로 출력을 뱉을때 입력의 배치사이즈를 고려해야하고, 출력하는 차원도 어떤차원인지 생각을 해보고 코드를 작성해야 하는것을 알게되었다. 특히 과제를 하며 데이터셋의 차원을 못맞춰서 에러뜨는경우가 여러번 있어서 차원을 맞추는 메소드나 함수를 복습하고 익혀야겠다.  

# 4. 학습회고
외국여행을 온 것같은 한주였다. 내가 하고 싶은게 있어도 버벅거리고, 왜이렇게 되는지 명확하게 이해가 가지 않는것들이 많았던것 같다. 머신러닝을 하는데 마치 언어처럼 다룰 수 있어야 하는게 PyTorch니깐 익숙해지도록 복습을 해봐야겠다.