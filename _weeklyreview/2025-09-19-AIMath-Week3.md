---
title: "Week3_AI_LifeCycle_학습회고"
date: 2025-09-19
tags:
  - AI LifeCycle
  - Transformer
  - Multi-head Self-Attention
excerpt: "3주차 AI LifeCycle내용 회고"
math: true
---

## 목차
1.  강의 복습 내용
2.  과제 결과물 정리
3.  학습 회고

---

## 1. 강의 복습 내용

### 1. 선형 모델과 학습의 원리

### 1-1. 선형 회귀와 분류

 **선형 회귀 (Linear Regression): 데이터 속 관계를 직선으로 표현하기**
    집값 예측 예시를 통해 독립변수(집 크기, 위치 등)가 종속변수(집값)에 어떻게 영향을 미치는지 직관적으로 이해할 수 있었다 모델의 신뢰도를 결정하는 네 가지 기본 가정, 즉 **선형성(Linearity), 독립성(Independence), 등분산성(Homoscedasticity), 정규성(Normality)**의 중요성을 시각 자료를 통해 명확히 확인했다
     특히 모델의 예측값과 실제값의 차이인 **잔차(residual)**($$y-\hat{y}$$)를 분석하여 이 가정들이 충족되었는지 확인하는 과정은, 모델의 성능을 넘어 모델이 데이터를 잘 설명하고 있는지를 근본적으로 이해하는 데 큰 도움이 되었다
     그리고 수많은 데이터 포인트 사이에서 오차를 최소화하는 최적의 직선을 찾는 **최소 제곱법(OLS, Ordinary Least Squares)**이 선형 회귀의 핵심 아이디어라는 것을 수식($$cost(m,b)=\Sigma(y_{i}-(mx_{i}+b))^{2}$$)을 통해 알게됐다.

 **최근접 이웃 분류기 (k-Nearest Neighbors, k-NN)**
    새로운 데이터가 주어졌을 때, 복잡한 계산 없이 주변의 가장 가까운 k개의 데이터를 보고 자신의 클래스를 결정하는 k-NN의 단순하면서도 강력한 아이디어가 인상 깊었다 학습 단계에서는 단순히 데이터를 기억하기만 하고($$O(1)$$), 예측 단계에서 모든 데이터와의 거리를 계산하는 구조를 가진다. 하지만 이러한 단순함 뒤에는 예측 시 모든 학습 데이터를 스캔해야 하는 비효율성($$O(N)$$)과 , 데이터의 차원이 증가할수록 데이터 간의 거리가 의미를 잃게 되는 **'차원의 저주(Curse of Dimensionality)'**때문에 잘 사용하지 않는다.,

 **선형 분류기 (Linear Classifier)**
   k-NN과 달리 학습 데이터 자체를 저장하는 것이 아니라, 입력(이미지 x)과 클래스 점수(y)를 매핑하는 함수 $$f(x,W)=Wx$$를 학습하는 가장 기본적인 딥러닝 방식이다. 학습이 끝나면 데이터셋 대신 학습된 가중치 행렬 $W$만 저장하면 되므로 공간 효율성이 높고, 예측 시에는 간단한 행렬 곱셈만으로 결과를 얻을 수 있어 속도도 훨씬 빠르다는 장점이 명확했다. 이는 마치 각 클래스에 대한 '템플릿'을 학습하는 것과 같다는 것을 시각 자료를 통해 이해할 수 있었다.
   아래 사진처럼 실제로 학습한걸 보면 뭔가 흐릿하게 보이는걸 알 수 있는데, 학습한 사진임을 알 수 있다.
![](/assets/images/2025-09-19-18-31-47.png)
### 1-2. 모델 학습의 핵심: 손실 함수와 최적화

 **Softmax와 확률적 해석**
    선형 분류기가 출력하는 점수(score)는 그 자체만으로는 해석이 어렵다는 한계가 있다. 이 문제를 **Softmax 함수**가 해결해 주었다. Softmax는 임의의 점수들을 0과 1 사이의 값으로 변환하고, 모든 클래스에 대한 값의 총합이 1이 되도록 만들어준다. 덕분에 모델의 출력을 각 클래스에 속할 '확률'로 해석할 수 있게 되어, 예측 결과에 대한 신뢰도를 높일 수 있었다

 **손실 함수 (Loss Function)**
    모델이 얼마나 좋은지 혹은 나쁜지를 정량적으로 평가하는 손실 함수의 중요성도 다시 한번 느꼈다. 예측값($$\hat{y}$$)과 실제값($$y$$)의 차이가 클수록 큰 페널티를 부여하는 방식으로 모델이 더 나은 방향으로 학습하도록 유도하는 핵심적인 역할을 한다는 것을 이해했다. 이진 분류에서 사용되는 Hinge Loss, Log Loss, Exponential Loss 등 다양한 손실 함수들이 어떻게 다른 방식으로 오차에 페널티를 부여하는지 그래프를 통해 비교하며 학습했다. 특히 널리 사용되는 **교차 엔트로피(Cross-Entropy)** 손실 함수는 두 확률 분포의 차이를 측정하는 방식으로, 모델의 예측 확률 분포가 실제 정답의 확률 분포와 같아지도록 학습을 유도한다는 것을 알게 됐다.

 **경사 하강법 (Gradient Descent)**
   손실 함수가 정의되면, 그 값을 최소화하는 최적의 매개변수(W)를 찾아야 한다. 이 과정을 **최적화(Optimization)**라고 부른다. 그중 가장 기본적이면서도 강력한 아이디어는 **경사 하강법(Gradient Descent)**이다. 이는 손실 함수라는 거대한 지형에서 현재 위치의 기울기(gradient)를 계산하고, 기울기가 가장 가파르게 내려가는 방향으로 매개변수를 조금씩 이동시켜 점차 최저점(minimum)을 찾아가는 원리다. 하지만 전체 데이터셋에 대해 기울기를 계산하는 것은 매우 비효율적일 수 있다. 이를 해결하기 위해 등장한 **확률적 경사 하강법(Stochastic Gradient Descent, SGD)**은 전체가 아닌 일부 데이터의 미니 배치(mini-batch)를 사용하여 기울기를 근사 계산함으로써 학습 속도를 크게 향상시켰다. 또한, SGD의 무작위적인 업데이트는 때로는 지역 최적점(local minimum)을 탈출하는 데 도움이 되기도 한다는 것을 배웠다.

---

### 2-1. 신경망의 구성 요소

* **활성화 함수 (Activation Functions)**
    단순히 선형 변환을 여러 번 반복하는 것은 결국 하나의 선형 변환과 같다. 신경망이 복잡하고 비선형적인 패턴을 학습할 수 있는 이유는 바로 각 층 사이에 존재하는 **활성화 함수** 덕분이다. 과거에 널리 쓰였던 **Sigmoid**나 **Tanh** 함수는 입력값이 너무 크거나 작을 때 기울기가 0에 가까워지는 **기울기 소실(Vanishing Gradient)** 문제를 가지고 있었다. 이 문제를 해결한 것이 바로 **ReLU(Rectified Linear Unit)** 함수다. ReLU는 양수 영역에서는 기울기가 1로 유지되어 기울기 소실 문제를 완화하고, 연산도 매우 효율적이라는 장점이 있다. 물론 ReLU 역시 음수 영역에서는 기울기가 0이 되어 뉴런이 '죽는(Dying ReLU)' 문제가 발생할 수 있으며, 이를 보완하기 위해 Leaky ReLU, ELU와 같은 다양한 변형 함수들이 등장했다는 발전 과정을 따라가 볼 수 있었다.

* **다층 퍼셉트론 (Multilayer Perceptron, MLP)**
   활성화 함수를 통해 비선형성을 추가한 여러 개의 선형 계층(fully connected layer)을 쌓아 올린 구조가 바로 다층 퍼셉트론, 즉 신경망이다. 각 층의 뉴런들은 이전 층의 출력을 입력으로 받아 자신만의 특징(feature)을 추출하고, 이렇게 추출된 특징들이 다음 층으로 전달되면서 점점 더 복잡하고 추상적인 특징을 학습하게 된다. 이를 통해 단일 선형 분류기로는 해결할 수 없었던 복잡한 비선형 결정 경계(decision boundary)를 만들어낼 수 있다.

### 2-2. 안정적인 학습을 위한 기법들

* **가중치 초기화 (Weight Initialization)**
    신경망 학습의 성패는 가중치를 어떻게 초기화하는지에서부터 갈릴 수 있다. 가중치가 너무 작게 초기화되면 신호가 층을 거치며 사라지고(vanishing activations) , 너무 크게 초기화되면 신호가 폭발하여 활성화 함수를 포화시켜(saturating activations) 학습이 제대로 이루어지지 않는다.**Xavier 초기화**나 He 초기화와 같은 방법들은 입력과 출력 노드의 수를 고려하여 가중치를 초기화함으로써, 층이 깊어져도 활성화 값의 분포가 안정적으로 유지되도록 돕는다. 이는 깊은 신경망을 효과적으로 훈련시키는 데 필수적인 기법임을 깨달았다.

* **데이터 전처리 및 증강 (Data Preprocessing and Augmentation)**
    모델의 성능을 극대화하기 위해서는 입력 데이터를 '잘' 다루는 것이 매우 중요하다. 데이터의 평균을 0으로 맞추는 **Zero-centering**과 표준편차로 나누어 분포를 조절하는 **정규화(Normalization)**는 학습 과정을 더 안정적이고 빠르게 만들어준다. 또한, 현실에서는 충분한 양의 데이터를 확보하기 어려운 경우가 많다. **데이터 증강(Data Augmentation)**은 기존의 이미지에 좌우 반전(horizontal flips), 무작위 자르기(random crops), 색상 변화(color jitter) 등 의미를 해치지 않는 선에서 변형을 가하여 학습 데이터의 양을 인위적으로 늘리는 기법이다. 이를 통해 모델은 다양한 변화에 대해 강건함(robustness)을 갖게 되고, 과적합(overfitting)을 방지하는 효과를 얻을 수 있다.

* **학습률 스케줄링 (Learning Rate Scheduling)**
    경사 하강법에서 매개변수를 업데이트하는 보폭의 크기인 **학습률(Learning Rate)**은 학습 과정에 지대한 영향을 미친다. 학습률이 너무 높으면 최적점을 지나쳐 발산할 수 있고, 너무 낮으면 학습이 매우 느려지거나 지역 최적점에 갇힐 수 있다. **학습률 스케줄링**은 학습이 진행됨에 따라 학습률을 동적으로 조절하는 전략이다. 일반적으로 학습 초기에는 비교적 큰 학습률로 빠르게 최적점 근처로 이동하고, 학습이 안정화되면 학습률을 점차 줄여나가 미세 조정을 통해 최적점에 더 잘 수렴하도록 한다 Step decay, Cosine decay, Linear decay와 같은 다양한 스케줄링 기법들이 있으며 , 학습 초기에 급격한 변화를 막기 위해 학습률을 서서히 증가시키는 '웜업(warmup)'기법을 사용함을 알게됐다.

---


### 3-1. 순환 신경망 (RNN)과 그 한계

* **RNN의 구조와 문제점**
    시계열 데이터 처리를 위해 고안된 **RNN(Recurrent Neural Network)**은 이전 타임 스텝의 은닉 상태(hidden state)를 다음 타임 스텝의 입력으로 재귀적으로 사용하는 독특한 구조를 가진다. 이를 통해 가변적인 길이의 시퀀스를 처리할 수 있고, 모델의 크기가 입력 길이에 따라 증가하지 않는다는 장점이 있다. 하지만 순차적으로 계산을 수행해야 하므로 병렬화가 어려워 학습 속도가 느리다는 단점이 있다. 더 심각한 문제는, 시퀀스가 길어질수록 동일한 가중치 행렬($$W_{hh}$$)이 반복적으로 곱해지면서 기울기가 0으로 수렴하거나 무한대로 발산하는 **기울기 소실/폭발(Vanishing/Exploding Gradient)** 문제가 발생한다는 것이다. 이는 RNN이 먼 과거의 정보를 현재까지 전달하기 어렵게 만들어, 장기 의존성(long-range dependency) 학습에 실패하는 근본적인 원인이 된다.

* **LSTM & GRU**
    RNN의 장기 기억 문제를 해결하기 위해 등장한 것이 바로 **LSTM(Long Short-Term Memory)**이다. LSTM은 기존의 은닉 상태($h_t$) 외에 **셀 상태(cell state, $c_t$)**라는 별도의 정보 흐름 경로를 두었다. 이 셀 상태는 '컨베이어 벨트'처럼 정보가 큰 변화 없이 오랫동안 흘러갈 수 있게 해주며, **Forget Gate, Input Gate, Output Gate**라는 세 개의 게이트가 이 정보의 흐름을 정교하게 제어한다. 이를 통해 어떤 정보를 잊고, 어떤 새로운 정보를 저장하며, 어떤 정보를 출력할지 학습함으로써 기울기 소실 문제없이 장기 의존성을 효과적으로 학습할 수 있게 되었다.**GRU(Gated Recurrent Unit)**는 LSTM의 구조를 단순화하여 유사한 성능을 내면서도 계산 효율성을 높인 모델이다.

#### 3-2. 어텐션과 트랜스포머의 등장

 ### Seq2Seq와 어텐션
기계 번역과 같은 많은-대-많은(many-to-many) 문제를 해결하기 위해 제안된 Seq2Seq 모델은 입력 시퀀스를 하나의 고정된 크기 벡터(context vector)로 압축하는 **인코더(Encoder)**와, 이 벡터를 바탕으로 출력 시퀀스를 생성하는 **디코더(Decoder)**로 구성된다. 하지만 긴 입력 문장의 모든 정보를 하나의 벡터에 담으려다 보니 정보 손실이라는 **병목 현상(bottleneck)**이 발생했다. 이 문제를 해결한 것이 바로 어텐션(Attention) 메커니즘이다. 어텐션은 디코더가 매 타임 스텝에서 출력 단어를 예측할 때, 인코더의 모든 은닉 상태를 다시 한번 참고하여 현재 예측에 가장 중요한 입력 단어에 '집중(attend)'하도록 가중치를 부여하는 방식이다. 이를 통해 모델은 더 이상 하나의 압축된 벡터에만 의존하지 않고, 필요할 때마다 입력 시퀀스의 특정 부분에 접근하여 정보를 가져올 수 있게 되었다.

### 트랜스포머
RNN 계열 모델들의 순차적인 계산 방식은 병렬 처리를 불가능하게 만들어 하드웨어의 성능을 온전히 활용하기 어려웠다. 2017년에 등장한 **트랜스포머(Transformer)**는 이러한 순환(recurrence) 구조를 완전히 제거하고, 오직 **셀프 어텐션(Self-Attention)**이라는 메커니즘만으로 입력 시퀀스 내의 관계를 파악하는 혁신적인 구조를 제안했다. 셀프 어텐션은 문장 내의 모든 단어들이 서로에게 어텐션을 수행하여 각 단어가 문맥 속에서 어떤 의미를 갖는지를 파악하는 과정이다. 또한, 여러 개의 어텐션을 병렬적으로 수행하는 Multi-Head Attention을 통해 다양한 관점에서 단어 간의 관계를 학습할 수 있게 했다. 순서 정보가 없는 문제를 해결하기 위해 각 단어의 위치 정보를 담은 **포지셔널 인코딩(Positional Encoding)**을 입력에 더해주는 정교함도 갖추었다.


### BERT와 ViT
트랜스포머의 강력한 성능은 자연어 처리를 넘어 다른 분야로 빠르게 확장되었다. **BERT(Bidirectional Encoder Representations from Transformers)**는 트랜스포머의 인코더 구조만을 활용하여 대규모 텍스트 데이터로 사전 학습된 언어 모델이다. 문장의 일부를 가리고 맞추는 **Masked Language Modeling(MLM)**과 두 문장이 이어지는지 예측하는 Next Sentence Prediction(NSP) 과제를 통해 문맥을 양방향으로 깊이 이해하는 능력을 학습했다. **Vision Transformer(ViT)**는 이미지를 여러 개의 작은 패치(patch)로 나누고, 이 패치들의 시퀀스를 트랜스포머의 입력으로 사용하여 이미지 분류를 수행하는 모델이다. 이는 트랜스포머 구조가 특정 도메인에 국한되지 않는 범용적인 시퀀스 처리 아키텍처임을 증명한 중요한 사례다.

## 2. 과제 결과물 정리

### 과제 1: Numpy로 구현한 선형 회귀 (Linear Regression)

첫 번째 과제는 Numpy를 이용해 대기오염 데이터셋의 PM10 농도로 PM2.5 농도를 예측하는 선형 회귀 모델을 만드는 것이었다.

### 핵심 구현 내용 및 코드

모델의 핵심은 **정규방정식(Normal Equation)**을 사용하여 최적의 가중치(weights)와 편향(bias)을 한 번에 계산하는 것이었다.

```python
# 정규방정식을 이용한 가중치 계산
self.weights = np.linalg.pinv(X.T @ X) @ X.T @ y
```

행렬 연산으로 한 번에 처리하기 위한 트릭이었다. 편향 b를 가중치 벡터 W에 포함시키기 위해, 모든 데이터 샘플에 항상 1의 값을 가지는 가상의 특성(x_0)을 추가했다. 이렇게 하면 수식은 y=W′X′ 형태로 단순화된다. 이 과정은 `np.hstack`를 통해 구현했다.

```python
# 모든 샘플에 bias 항을 위한 특성 '1'을 추가
X = np.hstack((np.ones((X.shape[0], 1)), X))
```

이렇게 계산된 가중치 벡터의 첫 번째 값 weights[0]이 바로 편향(bias)이 되고, 나머지가 실제 특성에 대한 가중치 weights[1:]가 된다.

### 느낀점 및 알게된 점

**편향(bias)의 의미**: 이론으로만 배우던 편향(y절편)을 왜 가중치 행렬에 포함시켜 계산하는지, 그리고 이를 위해 왜 모든 입력 데이터에 '1'을 추가하는지 그 트릭을 코드로 직접 구현하며 명확히 이해하게 됐다. 이는 단순히 계산의 편의성을 넘어, 모델이 데이터를 더 유연하게 표현할 수 있도록 하는 중요한 장치임을 깨달았다.

**데이터 전처리의 중요성**: dropna()를 이용해 결측치가 있는 행을 제거하고, train_test_split 함수를 직접 구현하며 데이터를 훈련용과 테스트용으로 나누는 과정의 중요성을 다시 한번 느꼈다. 모델의 성능은 결국 데이터의 질에 달려있다는 것을 알게됐다.

### 과제 2: Numpy로 구현한 역전파 (Backpropagation)

두 번째 과제는 MNIST 손글씨 데이터셋을 분류하는 2-Layer 신경망을 Numpy만으로 구현하는 것이었다. 순전파(Forward Propagation)와 역전파(Backpropagation)를 직접 코딩해야 했다.

### 핵심 구현 내용 및 코드

모델의 구조는 다음과 같은 수식으로 표현된다.
$$\hat{Y}=softmax(σ(X⋅W1​+b1​)⋅W2​+b2​)$$

가중치 초기화는 Xavier 초기화 방법을 사용했다. 이는 각 층의 입출력 뉴런 수에 맞춰 가중치의 초기 분산을 조절하여, 학습 과정에서 발생할 수 있는 기울기 소실(vanishing gradient)이나 폭주(exploding gradient) 문제를 완화하기 위함이다. 균등분포를 사용할 경우, 초기화 범위는 다음과 같이 설정된다.
$$limit=\frac{6}{n_{in​}+n_{out​}}​​$$

```python
# Xavier 초기화를 사용한 가중치 행렬 생성
 W1 = np.random.uniform(
     low=-np.sqrt(6 / (input_dim + num_hiddens)),
     high=np.sqrt(6 / (input_dim + num_hiddens)),
     size=(input_dim, num_hiddens)
 )
```

역전파 과정에서는 출력층에서부터 입력층 방향으로 손실 함수의 기울기를 **연쇄 법칙(Chain Rule)**에 따라 계산해 나갔다. 예를 들어, 출력층의 가중치 W_2에 대한 기울기는 다음과 같이 구했다.

```python
# 역전파 구현
 dl_dz2 = (ff_dict['y'] - Y) / X.shape[0]  # Softmax와 Cross-Entropy 손실의 미분
 dl_dW2 = ff_dict['a1'].T @ dl_dz2
 dl_db2 = np.sum(dl_dz2, axis=0)
```

### 느낀점 및 알게된 점

**하이퍼파라미터**: 처음에는 학습률(learning rate)을 0.00001로 매우 작게 설정했더니 학습이 거의 진행되지 않았다. 이를 0.01로 높이자 손실이 눈에 띄게 감소하며 학습이 원활히 진행되는 것을 보고 하이퍼파라미터 튜닝의 중요성을 체감했다.

**활성화 함수**: 처음에는 활성화 함수로 Sigmoid를 사용했는데, 최종 테스트 정확도가 약 91.5%에 그쳤다. 이후 ReLU로 바꾸어 다시 학습시켜보니, 학습 속도가 더 빨라지고 최종 정확도도 95.2%까지 향상됐다. 이는 Sigmoid 함수가 특정 구간에서 기울기가 0에 가까워져 발생하는 기울기 소실 문제 때문임을 이론과 실제 결과를 통해 명확히 알게 됐다.

**오버플로우 방지**: Softmax 함수를 구현할 때, 지수 함수 np.exp()의 특성상 입력값이 조금만 커져도 오버플로우가 발생할 수 있다. 이를 방지하기 위해 입력 벡터의 최댓값을 각 원소에서 빼주는 기법을 사용했다. 이런 작은 트릭 하나가 코드의 안정성을 크게 좌우한다는 것을 배웠다.

### 과제 3: 멀티 헤드 셀프 어텐션 구현 (Multi-head Self-Attention)

핵심 구현 내용 및 코드

어텐션의 기본 수식은 다음과 같다.
$$Attention(Q,K,V)=softmax(\frac{Q\cdot K^T}{d_k​​}​)V$$

멀티 헤드 어텐션은 hidden_size를 num_heads 개수만큼으로 쪼개어 여러 개의 어텐션을 병렬로 수행하는 방식이다. 이를 위해 tp_attn (Transpose for Attention)이라는 함수를 통해 텐서의 차원을 재배열하는 과정이 필수적이었다.

예를 들어, [batch_size, seq_len, hidden_size] 형태의 텐서를 [batch_size, num_heads, seq_len, attn_head_size] 형태로 바꾸어 각 헤드가 독립적으로 어텐션 계산을 수행할 수 있도록 했다.
```python

# 텐서의 차원을 어텐션 계산에 맞게 재배열하는 함수
 def tp_attn(self, x):
     # (batch, seq_len, hidden_size) -> (batch, seq_len, num_heads, attn_head_size)
     x_shape = x.size()[:-1] + (self.num_heads, self.attn_head_size)
     x = x.view(*x_shape)
    # -> (batch, num_heads, seq_len, attn_head_size)
     return x.permute(0, 2, 1, 3)
```
이후 각 헤드별로 계산된 어텐션 결과를 다시 합치고 nn.Linear 층을 통과시켜 최종 출력을 얻는다.

### 느낀점 및 알게된 점

**텐서 차원 조작의 중요성**: 멀티 헤드 어텐션의 핵심은 텐서의 view와 permute를 이용한 차원 변환에 있다는 것을 깨달았다. hidden_size를 여러 개의 attn_head_size로 나누어 병렬 처리함으로써, 모델이 입력 시퀀스의 다양한 부분 공간(subspace)에서 정보를 동시에 학습할 수 있게 된다는 개념을 코드를 통해 직관적으로 이해할 수 있었다.

**수식과 코드의 연결**: Q, K, T 계산을 위해 `torch.matmul(Q_layer, K_layer.transpose(-1, -2)) `코드를 작성하며, 행렬 곱셈을 위해 키(K) 텐서의 마지막 두 차원을 전치(transpose)해야 하는 이유를 명확히 알게 됐다. 수식으로만 보던 개념이 실제 코드에서 어떻게 구현되는지 알 수 있었다.

## 3. 소감
 내 포트폴리오를 정리하고, 내가 공부한 내용들을 정리할 블로그 하나 갖고 싶었다. 드디어 깃허브에 나만의 블로그를 만들어 정착한것 같아 기분이 좋다. 이번주에 풀이한 알고리즘 문제들과 과제풀이들도 포스팅해 올렸고, 앞으로도 올릴 예정이다. 
 다만 이번주는 블로그에 내용정리하느라 시간을 너무 많이써서 복습에 소홀했다. 앞으로는 복습을 먼저 우선적으로 하고 정리를 해야겠다.
 
 Transformer를 공부하며 ViT에 대해 궁금한점이 많아졌다. 이미지를 Sequence Data로 이용해서 기존의 CNN모델을 제치고 어떻게 SOTA를 달성했는지 궁금하다. 이번 주말에는 이번주에 배운내용을 복습하며 ViT논문을 공부해야겠다.

 요즘 이런저런 이걸 해볼까, 이건 어떨까 고민, 생각만 깊게하고 뭔가 제대로 이뤄낸건 없는 것 같다. 일단 완벽하진 않더라도 시작부터 하고, 틀린게 있으면 그때부터 고쳐나가면 좀 더 발전하지 않을까 싶다.
   
