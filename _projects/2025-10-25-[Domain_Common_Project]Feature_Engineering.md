---
layout: single
title: "[Domain_Common_Project][ëª¨ë¸ìµœì í™”]-Feature_Engineering"
date: 2025-10-25
tags:
  - Domain_Common_Project
  - study
  - Feature_Engineering
excerpt: "[ëª¨ë¸ìµœì í™”]-Feature_Engineering."
math: true
---


# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ë¦¬

- íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì •ê·œí™”
- í•„ìš”ì‹œ í•œêµ­ì–´ íŠ¹í™” ì „ì²˜ë¦¬ ì²˜ë¦¬
- ë§¤ìš° ì§§ê±°ë‚˜ ê¸´ í…ìŠ¤íŠ¸ ì œê±° ë˜ëŠ” ì²˜ë¦¬
- ì¤‘ë³µ ì œê±°

![image](/assets/images/2025-10-25-18-37-59.png)

- ììŒ, ëª¨ìŒë§Œ ìˆëŠ”ê²½ìš°ë¥¼ ì œê±°í•œë‹¤.
- ë°˜ë³µë˜ëŠ” í‘œí˜„ì„ ì •ê·œí™”í•œë‹¤. ì¦‰ â€œã…‹ã…‹ã…‹ã…‹â€ë‚˜ â€œã…ã…ã…ã…ã…â€ê°™ì€ í‘œí˜„ì„ â€œã…‹ã…‹â€, â€œã…ã…â€í˜•íƒœë¡œ ì •ê·œí™” í•œë‹¤.
- ë¹„ì–´ìˆëŠ” í…ìŠ¤íŠ¸ëŠ” ì œê±°í•œë‹¤.
- ì¤‘ë³µë˜ëŠ” ë°ì´í„°ë„ ëª¨ë‘ ì œê±°í•œë‹¤.

# í…ìŠ¤íŠ¸ ì •ê·œí™”

- ëŒ€ì†Œë¬¸ì ì •ê·œí™”
- êµ¬ë‘ì  ì²˜ë¦¬
- íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
- URL/ì´ë©”ì¼/ë©˜ì…˜ ì •ë¦¬

```python
print("í…ìŠ¤íŠ¸ ì •ê·œí™” ê³¼ì • ì‹œì‘")
print("=" * 50)

# ëŒ€ì†Œë¬¸ì ì •ê·œí™” (BERTê°€ ì²˜ë¦¬í•˜ì§€ë§Œ ì¼ê´€ì„±ì„ ìœ„í•´)
print("\n1ë‹¨ê³„: ëŒ€ì†Œë¬¸ì ì •ê·œí™” ìˆ˜í–‰ ì¤‘...") 
df_processed["review_normalized"] = df_processed["review_cleaned"].str.lower()
print("âœ“ ì†Œë¬¸ì ë³€í™˜ ì™„ë£Œ")

# êµ¬ë‘ì  ì •ê·œí™”
print("\n2ë‹¨ê³„: êµ¬ë‘ì  ì •ê·œí™” ìˆ˜í–‰ ì¤‘...")

def normalize_punctuation(text):
    # ì—¬ëŸ¬ ê°œì˜ êµ¬ë‘ì ì„ í•˜ë‚˜ë¡œ ì •ê·œí™”
    # 1. (ìˆ˜ì •) ì¤‘ë¦½ êµ¬ë‘ì (.,)ì˜ ê³¼ë„í•œ ë°˜ë³µì„ 1ê°œë¡œ ì¶•ì†Œ (e.g., "..." -> ".")
    #    BERTì˜ ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ì œê±°í•˜ì§€ ì•Šê³  'ìœ ì§€'í•©ë‹ˆë‹¤.
    text = re.sub(r"([.,])\1{2,}", r"\1", text) 
    # 2. ê°ì„± êµ¬ë‘ì (!, ?)ì˜ ê³¼ë„í•œ ë°˜ë³µì„ 2ê°œë¡œ ì¶•ì†Œ (e.g., "!!!!" -> "!!")
    text = re.sub(r"([!?])\1{2,}", r"\1\1", text) 
    text = re.sub(r"[,]{2,}", ",", text)
    
    #3. (ê°•í™”) ê°ì„±ê³¼ ê´€ë ¨ ì—†ëŠ” íŠ¹ì • ë…¸ì´ì¦ˆ ë¬¸ì ì œê±° (í™•ì¥ëœ Blacklist ë°©ì‹)
    #    í•´ì‹œíƒœê·¸, ë¶ˆí•„ìš”í•œ ê°•ì¡°, ë¦¬ìŠ¤íŠ¸, ìˆ˜í•™/í™”í ê¸°í˜¸, ì¼ë¶€ ê´„í˜¸ ë“±
    #    (â™¥, ğŸ‘, ^^, >_< ê°™ì€ ì´ëª¨í‹°ì½˜/emojiëŠ” ì´ ëª©ë¡ì— ì—†ìœ¼ë¯€ë¡œ 'ìœ ì§€'ë©ë‹ˆë‹¤.)
    text = re.sub(
        r"[#*â€¢â– â—†â—‡â–¡â—‹â—â€»+=/\\%&{}\[\]$â‚©â‚¬Â£Â¥|`]",  # <-- ëŒ€í­ í™•ì¥ëœ ë…¸ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸
        " ",
        text
    )
    # ì •ê·œì‹ ì„¤ëª…:
    # [#*â€¢â– â—†â—‡â–¡â—‹â—â€»] : í•´ì‹œíƒœê·¸, ë¶ˆë¦¿, íŠ¹ìˆ˜ ê¸°í˜¸
    # [+=/\\%&]       : ìˆ˜í•™/ê¸°ìˆ  ê¸°í˜¸ ( \ëŠ” \\ë¡œ ì´ìŠ¤ì¼€ì´í”„)
    # [{}\[\]]         : ì´ëª¨í‹°ì½˜ì— ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ê´„í˜¸ ( []ëŠ” \ë¡œ ì´ìŠ¤ì¼€ì´í”„)
    # [$â‚©â‚¬Â£Â¥]          : í™”í ê¸°í˜¸
    # [|`]             : íŒŒì´í”„, ë°±í‹±

    # 4. (ìˆ˜ì •) ì „ì²´ êµ¬ë‘ì  ì£¼ë³€ ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+([.,!?])", r"\1", text)  # "ì •ë§ !" -> "ì •ë§!"
    text = re.sub(r"([.,!?])\s+", r"\1 ", text) # "ìµœê³ ! !!" -> "ìµœê³ ! !! "

    # 5. ê³¼ë„í•œ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ìµœì¢… ì •ë¦¬
    text = re.sub(r"\s+", " ", text)

    return text.strip()

df_processed["review_normalized"] = df_processed["review_normalized"].apply(
    normalize_punctuation
)
print("âœ“ êµ¬ë‘ì  ì •ê·œí™” ì™„ë£Œ")

# íŠ¹ìˆ˜ë¬¸ì ì¶”ê°€ ì •ë¦¬
print("\n3ë‹¨ê³„: íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ ìˆ˜í–‰ ì¤‘...")

def clean_special_chars(text):
    # URL íŒ¨í„´ ì œê±° (ìˆëŠ” ê²½ìš°)
    text = re.sub(
        r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
        " ",
        text,
    )

    # ì´ë©”ì¼ íŒ¨í„´ ì œê±° (ìˆëŠ” ê²½ìš°)
    text = re.sub(r"\S+@\S+", " ", text)

    # ë©˜ì…˜ íŒ¨í„´ ì œê±° (ìˆëŠ” ê²½ìš°)
    text = re.sub(r"@\w+", " ", text)

    # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+", " ", text)

    return text.strip()

df_processed["review_normalized"] = df_processed["review_normalized"].apply(
    clean_special_chars
)
print("âœ“ URL/ì´ë©”ì¼/ë©˜ì…˜ ì œê±° ì™„ë£Œ")

# ì •ê·œí™” í›„ ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
print("\n4ë‹¨ê³„: ì •ê·œí™” í›„ ë¹ˆ í…ìŠ¤íŠ¸ í™•ì¸ ì¤‘...")
empty_after_normalization = df_processed["review_normalized"].str.strip().eq("").sum()
if empty_after_normalization > 0:
    df_processed = df_processed[df_processed["review_normalized"].str.strip() != ""]
    print(f"âœ“ ë¹ˆ í…ìŠ¤íŠ¸ {empty_after_normalization}ê°œ ì œê±°")
else:
    print("âœ“ ë¹ˆ í…ìŠ¤íŠ¸ ì—†ìŒ")

# ì •ê·œí™” ì „í›„ ë¹„êµ
print("\n" + "=" * 50)
print("ì •ê·œí™” ê²°ê³¼ ìš”ì•½:")
print(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {len(df_processed):,}ê°œ")
print(f"í‰ê·  ê¸¸ì´ - ì •ë¦¬ë¨: {df_processed['review_cleaned'].str.len().mean():.1f}ì")
print(
    f"í‰ê·  ê¸¸ì´ - ì •ê·œí™”ë¨: {df_processed['review_normalized'].str.len().mean():.1f}ì"
)
print("=" * 50)
```

- ëª¨ë“ ë¬¸ìë¥¼ ìš°ì„   ì†Œë¬¸ìë¡œ ì •ê·œí™” í•œë‹¤.
- êµ¬ë‘ì ì²˜ë¦¬
    - ì¤‘ë³µëœ êµ¬ë‘ì ì€ 1ê°œë‚˜ 2ê°œë¡œ ì¶•ì†Œí•´ì„œ ì •ê·œí™”í•œë‹¤.
    - ê°ì„±ê³¼ ê´€ë ¨ì—†ëŠ” íŠ¹ìˆ˜ë¬¸ìëŠ” ë…¸ì´ì¦ˆë¡œ ì—¬ê²¨ì„œ BlackListí˜•ì‹ìœ¼ë¡œ ì œê±°í•œë‹¤.
    - ê°ì„±ê³¼ ê´€ë ¨ìˆëŠ” ì´ëª¨í‹°ì½˜ì´ë‚˜, íŠ¹ìˆ˜ë¬¸ìëŠ” ëª©ë¡ì— ë„£ì§€ ì•Šì•„ì„œ ìœ ì§€ëœë‹¤.
- êµ¬ë‘ì  ì£¼ë³€ ê³µë°±ì„ ì •ë¦¬í•œë‹¤.

- URLíŒ¨í„´ì´ë‚˜ ì´ë©”ì¼íŒ¨í„´, ë©˜ì…˜íŒ¨í„´, ê³¼ë„í•œ ê³µë°±ì€ ê°ì„±ë¶„ë¥˜ì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” ë…¸ì´ì¦ˆì´ë¯€ë¡œ ì œê±°í•œë‹¤.

![image](/assets/images/2025-10-25-18-38-15.png)

BaseLineì½”ë“œì—ì„œëŠ” ìµœëŒ€í•œ ììŒ/ëª¨ìŒë§Œ ë°˜ë³µëœ íŒ¨í„´, íŠ¹ìˆ˜ë¬¸ì, ì´ëª¨ì§€ë“±ì„ ëª¨ë‘ ë…¸ì´ì¦ˆì²˜ë¦¬í•´ì„œ ì „ì²˜ë¦¬í–ˆì—ˆë‹¤. í•˜ì§€ë§Œ ì´ê²ƒë“¤ì´ ê°ì„±ë¶„ë¥˜ì— ë„ì›€ì´ ë ìˆ˜ë„ ìˆì§€ ì•Šì„ê¹Œë¼ëŠ” ìƒê°ì— í•´ë‹¹ íŠ¹ìˆ˜ë¬¸ì/ ì´ëª¨ì§€/ ê°ì •ì„ ë‚˜íƒ€ë‚´ëŠ” íŒ¨í„´ë“±ì„ íŠ¹ìˆ˜í† í°ìœ¼ë¡œ ë§¤í•‘í•´ì„œ ìµœëŒ€í•œ ì‚¬ìš©í•˜ê³ ì ìœ„ì˜ ì½”ë“œì²˜ëŸ¼ êµ¬í˜„í–ˆë‹¤.

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¬êµ¬ì„±

- ì•ì„œ ì •ì˜í•œ í•¨ìˆ˜ë“¤ì„ ì´ìš©í•˜ì—¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
- DataLeakageë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ ì „ëµì„ í¬í•¨í•´ì•¼ í•œë‹¤.
    - ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµ ë°ì´í„°ì—ì„œë§Œ í•™ìŠµ
    - Validation/Testë°ì´í„°ëŠ” í•™ìŠµëœ íŒŒë¼ë¯¸í„°ë¡œë§Œ ë³€í™˜
    - ë ˆì´ë¸” ì •ë³´ë¥¼ í™œìš©í•œ ì „ì²˜ë¦¬ëŠ” í•™ìŠµ ì‹œì—ë§Œ ì ìš©

```python
class TextPreprocessingPipeline:
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤ (ì •ê·œì‹ ì˜¤ë¥˜ ìˆ˜ì •)
    - ì „ì²˜ë¦¬ ë¡œì§ì„ ë‹¨ì¼ ë©”ì„œë“œë¡œ í†µí•© ë° ìµœì í™”
    - fit ë©”ì„œë“œ: ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €ë¥¼ ìœ„í•œ ë¼ë²¨ë³„ ê°ì„± íŒ¨í„´ ìˆ˜ì§‘
    """

    def __init__(self):
        self.is_fitted = False
        # fitì˜ ê²°ê³¼ë¬¼: ë¼ë²¨ë³„ ìƒìœ„ ê°ì„± í‘œí˜„(ì´ëª¨í‹°ì½˜/emoji)ì„ ì €ì¥
        self.emotion_patterns_by_label = {}
        
        # ê°ì„± í‘œí˜„ì„ ì°¾ëŠ” ì •ê·œì‹
        # 1. (ã…‹ã…ã… ã…œã…¡): í•œêµ­ì–´ ê°ì • í‘œí˜„ (e.g., ã…‹ã…‹, ã… ã… )
        # 2. (T^>c<;:=_...): ì•„ìŠ¤í‚¤ ì´ëª¨í‹°ì½˜ (e.g., ^_^, T_T, :), >_<)
        # 3. (\U0001F...): ìœ ë‹ˆì½”ë“œ ì´ëª¨ì§€ (e.g., ğŸ˜‚, ğŸ‘, â™¥)
        
        # --- [ìˆ˜ì •ëœ ë¶€ë¶„] ---
        self.EMOTION_REGEX = re.compile(
            r'([ã…‹ã…ã… ã…œã…¡]{2,})|'  # 1. í•œêµ­ì–´
            
            # 2. ì•„ìŠ¤í‚¤ (ë§¨ ëì— ëˆ„ë½ëœ ')'ë¥¼ '|' ì•ì— ì¶”ê°€)
            r'([T^>c<;:=_]{1,}(?:[-_oO\']{0,1})(?:[\)\]DdpP]|\[<]{1,}))|'
            
            # 3. ì´ëª¨ì§€ (ìº¡ì²˜ ê·¸ë£¹ '()'ìœ¼ë¡œ ë¬¶ê³  ë§¨ ë’¤ì˜ ë¶ˆí•„ìš”í•œ ')' ì œê±°)
            r'([\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\u2600-\u26FF\u2700-\u27BFâ™¥ğŸ‘ğŸ˜‚])'
        )
        # --- [ìˆ˜ì • ì™„ë£Œ] ---

    def _clean_noise_patterns(self, text: str) -> str:
        """[1ìˆœìœ„] URL, ì´ë©”ì¼, ë©˜ì…˜ ë“± íŒ¨í„´ ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°"""
        text = re.sub(
            r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
            " ", text
        )
        text = re.sub(r"\S+@\S+", " ", text)
        text = re.sub(r"@\w+", " ", text)
        return text

    
    def _apply_preprocessing_rules(self, text: str) -> str:
        """
        [í•µì‹¬] ê¸°ë³¸ ì „ì²˜ë¦¬ ë° ì •ê·œí™” ê·œì¹™ì„ 'ì˜¬ë°”ë¥¸ ìˆœì„œ'ë¡œ ì ìš©
        """
        if pd.isna(text):
            return ""

        text = str(text).strip()

        # 1. (ê°€ì¥ ë¨¼ì €) URL, ì´ë©”ì¼, ë©˜ì…˜ ë“± ë…¸ì´ì¦ˆ íŒ¨í„´ ì œê±°
        text = self._clean_noise_patterns(text)

        # 2. ì†Œë¬¸ìí™” (BERTê°€ ì²˜ë¦¬í•˜ì§€ë§Œ ì¼ê´€ì„±ì„ ìœ„í•´)
        text = text.lower()

        # 3. (ê°•í™”) ê°ì„±ê³¼ ê´€ë ¨ ì—†ëŠ” íŠ¹ì • ë…¸ì´ì¦ˆ ë¬¸ì ì œê±° (Blacklist)
        text = re.sub(
            r"[#*â€¢â– â—†â—‡â–¡â—‹â—â€»+=/\\%&{}\[\]$â‚©â‚¬Â£Â¥|`]", " ", text
        )
        
        # 4. í•œêµ­ì–´ íŠ¹í™” ì •ë¦¬ (ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ê²½ìš°)
        # text = re.sub(r"[ã„±-ã…ã…-ã…£]+", "", text)
        
        # 5. ê°ì • í‘œí˜„ ì •ê·œí™” (e.g., ã…‹ã…‹ã…‹ -> ã…‹ã…‹, ã… ã… ã…  -> ã… ã… )
        text = re.sub(r"([ã…‹ã…])\1{2,}", r"\1\1", text) 
        text = re.sub(r"([ã… ã…œã…¡])\1{2,}", r"\1\1", text)
        
        # 6. êµ¬ë‘ì  ì •ê·œí™”
        text = re.sub(r"([.,])\1{2,}", r"\1", text)     # e.g., "..." -> "."
        text = re.sub(r"([!?])\1{2,}", r"\1\1", text)  # e.g., "!!!!" -> "!!"

        # 7. êµ¬ë‘ì  ì£¼ë³€ ê³µë°± ì •ë¦¬
        text = re.sub(r"\s+([.,!?])", r"\1", text)
        text = re.sub(r"([.,!?])\s+", r"\1 ", text)

        # 8. (ë§ˆì§€ë§‰) ê³¼ë„í•œ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ìµœì¢… ì •ë¦¬
        text = re.sub(r"\s+", " ", text)
        
        return text.strip()

    def fit(self, texts: pd.Series, labels: pd.Series):
        """
        í•™ìŠµ ë°ì´í„°(texts, labels)ë¡œë¶€í„° ì „ì²˜ë¦¬ ì •ë³´(ê°ì„± íŒ¨í„´)ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
        """
        print("í•™ìŠµ ë°ì´í„° ê¸°ë°˜ ê°ì„± íŒ¨í„´ ìˆ˜ì§‘ ì¤‘...")
        
        if not isinstance(texts, pd.Series):
            texts = pd.Series(texts)
        if not isinstance(labels, pd.Series):
            labels = pd.Series(labels)

        # 1. ë¶„ì„ ì „, í…ìŠ¤íŠ¸ì— ê¸°ë³¸ ì •ê·œí™” ê·œì¹™ì„ ë¨¼ì € ì ìš©í•©ë‹ˆë‹¤.
        processed_texts = texts.apply(self._apply_preprocessing_rules)
        
        df = pd.DataFrame({'text': processed_texts, 'label': labels})

        # 2. ë¼ë²¨ë³„ë¡œ ìˆœíšŒí•˜ë©° ê°ì„± íŒ¨í„´(ì´ëª¨í‹°ì½˜, emoji ë“±)ì„ ì°¾ìŠµë‹ˆë‹¤.
        for label in sorted(df['label'].unique()):
            label_texts = df[df['label'] == label]['text']
            
            # findallê³¼ explodeë¥¼ ì‚¬ìš©í•´ ëª¨ë“  ê°ì„± í‘œí˜„ì„ ì¶”ì¶œ
            all_emotions = label_texts.str.findall(self.EMOTION_REGEX).explode().dropna()
            
            # íŠœí”Œë¡œ ë°˜í™˜ë˜ëŠ” ì •ê·œì‹ ê²°ê³¼ë¥¼ (e.g., ('ã…‹ã…‹', '', '')) í•˜ë‚˜ë¡œ í•©ì¹¨
            # (íŠœí”Œì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ë°©ì§€)
            cleaned_emotions = []
            for tup in all_emotions:
                found = next((item for item in tup if item), None)
                if found:
                    cleaned_emotions.append(found)

            # 3. ë¹ˆë„ë¥¼ ê³„ì‚°í•˜ê³  ìƒìœ„ 10ê°œë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
            top_emotions = Counter(cleaned_emotions).most_common(10)
            self.emotion_patterns_by_label[label] = top_emotions
            
        self.is_fitted = True
        print("âœ“ ê°ì„± íŒ¨í„´ ìˆ˜ì§‘ ì™„ë£Œ (íŒŒì´í”„ë¼ì¸ í•™ìŠµ ì™„ë£Œ)")
        
        # ìˆ˜ì§‘ëœ ì •ë³´ ì˜ˆì‹œ ì¶œë ¥
        print("\n--- ìˆ˜ì§‘ëœ ë¼ë²¨ë³„ ìƒìœ„ ê°ì„± íŒ¨í„´ (ì˜ˆì‹œ) ---")
        for label, patterns in self.emotion_patterns_by_label.items():
            print(f"  [Label {label}]: {patterns}")
        print("------------------------------------------")

    def transform(self, texts: pd.Series) -> pd.Series:
        """
        í•™ìŠµëœ íŒŒì´í”„ë¼ì¸(ë˜ëŠ” ê¸°ë³¸ íŒŒì´í”„ë¼ì¸)ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        if not self.is_fitted:
            print(
                "Warning: íŒŒì´í”„ë¼ì¸ì´ 'fit'ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì „ì²˜ë¦¬ ê·œì¹™ë§Œ ì ìš©í•©ë‹ˆë‹¤."
            )
            
        if not isinstance(texts, pd.Series):
            texts = pd.Series(texts)

        # _apply_preprocessing_rules í•¨ìˆ˜ë¥¼ .applyë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì ìš©
        processed_texts = texts.apply(self._apply_preprocessing_rules)
        
        return processed_texts

    def fit_transform(self, texts: pd.Series, labels: pd.Series) -> pd.Series:
        """í•™ìŠµê³¼ ë³€í™˜ì„ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        self.fit(texts, labels)
        return self.transform(texts)

#íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
preprocessor = TextPreprocessingPipeline()
```

- ì´ì „ì— ì •ì˜í•œ í•¨ìˆ˜ë“¤ì„ ì´ìš©í•´ì„œ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ Classë¥¼ ìƒì„±í•œë‹¤.
    - `clean_noise_pattens()` `,_apply_preprocessing_rules()` í•¨ìˆ˜ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•œë‹¤.
- `fit()` ë©”ì„œë“œì—ì„œëŠ” ì´í›„ k-foldë°©ì‹ìœ¼ë¡œ ë‚˜ëˆŒ ë°ì´í„°ë“¤ì˜ íŠ¹ì§•ì„ ì‹œê°í™”í•˜ê³ , í™•ì¸í•˜ëŠ” ë°©ë²•ë“¤ì„ ì‚¬ìš©í•œë‹¤. â†’ ìƒìœ„ 10ê°œ ì €ì¥
    - ë°ì´í„°ë“¤ì˜ íŠ¹ì§•(ì´ëª¨ì§€ ë¹ˆë„, ê°ì„±í‘œí˜„ â€˜ã…‹ã…‹â€™ë“±ì˜ ë¹ˆë„)ì„ í™•ì¸í•˜ê³  í† í¬ë‚˜ì´ì €ì— íŠ¹ìˆ˜í† í°ì„ ì¶”ê°€í•œë‹¤.
    - íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ `preprocessor`ë¡œ ì„ ì–¸í•œë‹¤.

# BERT ì „ìš© ì „ì²˜ë¦¬ í™•ì¸

- BERT í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•´ì„œ í† í°í™”
- í† í°í™” ê¸¸ì´ ì œí•œ ì²˜ë¦¬(BERTëŠ” 512 ì´í•˜ í† í°ìœ¼ë¡œ ì œí•œ)
- íŠ¹ìˆ˜ í† í° ì¶”ê°€([CLS],[SEP])
- ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
- ì ˆë‹¨ ë° íŒ¨ë”© ì „ëµ ì²˜ë¦¬

```python
# BERT í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
# model_name = "klue/bert-base"
model_name = "beomi/kcbert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

print(
    f"BERT í† í¬ë‚˜ì´ì €: {model_name}, vocab: {tokenizer.vocab_size}, max_length: {tokenizer.model_max_length}"
)
```

ëª¨ë¸ ì œí•œì— ë”°ë¼ì„œ ì œí•œëœ ëª¨ë¸ì¤‘ ì¼ë‹¨ `"beomi/kcbert-base"` ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤. ì´ ëª¨ë¸ì€ **ì˜¨ë¼ì¸ ëŒ“ê¸€, ë‰´ìŠ¤ ë¦¬ë·° ë“± ë¹„ì •í˜•ì ì¸ í…ìŠ¤íŠ¸**ë¥¼ í¬í•¨í•œ ëŒ€ìš©ëŸ‰ì˜ í•œêµ­ì–´ ë°ì´í„°ë¡œ í•™ìŠµë˜ì–´ìˆë‹¤ê³  í•œë‹¤. ë”°ë¼ì„œ ì˜í™”ë¦¬ë·°ë¥¼ ë¶„ì„í• ë•Œ, ì‹ ì¡°ì–´, ì´ëª¨ì§€, êµ¬ì–´ì²´ ë¬¸ì¥ì— ëŒ€í•œ ì´í•´ë„ê°€ ê°€ì¥ ë†’ì„ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•œë‹¤.

```python
# --- 1ë‹¨ê³„: íŠ¹ìˆ˜ í† í° ë° ì¹˜í™˜ ê·œì¹™ ì •ì˜ ---
# ì´ ê·œì¹™ì€ preprocessor.fit()ì˜ í†µê³„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ê°œë°œìê°€' ì •ì˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
# (ì˜ˆì‹œ: {'Label 0': [('ã… ã… ', 5000)], 'Label 3': [('ã…‹ã…‹', 8000), ('â™¥', 500)]})

# 1. ìƒˆë¡œ ì¶”ê°€í•  íŠ¹ìˆ˜ í† í° ë¦¬ìŠ¤íŠ¸
# (ëª¨ë¸ì´ ì´ í† í°ì„ í•˜ë‚˜ì˜ ì˜ë¯¸ ë‹¨ìœ„ë¡œ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤)
NEW_SPECIAL_TOKENS = [
    "[LAUGH]",  # ì›ƒìŒ (ã…‹ã…‹, ã…ã… ë“±)
    "[CRY]",    # ìš¸ìŒ (ã… ã… , ã…œã…œ ë“±)
    "[SMILE]",  # ë¯¸ì†Œ (^^, :) ë“±)
    "[HEART]",  # í•˜íŠ¸ (â™¥, â™¡ ë“±)
    "[EMOJI]",  # ê¸°íƒ€ ì´ëª¨ì§€ (ğŸ‘, ğŸ˜‚ ë“±)
]

# 2. ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ìœ„ íŠ¹ìˆ˜ í† í°ìœ¼ë¡œ ì¹˜í™˜í•˜ê¸° ìœ„í•œ ë§¤í•‘ (ì •ê·œì‹)
# (ìˆœì„œê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. ë” ê¸´ ê²ƒì„ ë¨¼ì € ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. e.g., ã… ã…  > ã… )
EMOTION_MAP = {
    re.compile(r'([ã…‹ã…]){2,}'): "[LAUGH]",  # ã…‹ã…‹, ã…ã…, ã…‹ã…‹ã…‹ã…‹ ë“±
    re.compile(r'([ã… ã…œ]){2,}'): "[CRY]",    # ã… ã… , ã…œã…œ, ã… ã… ã…  ë“±
    re.compile(r'(\^{2,})|(\:\))|(\:D)'): "[SMILE]", # ^^, :), :D ë“±
    re.compile(r'[â™¥â™¡]'): "[HEART]",        # â™¥, â™¡
    # ê¸°íƒ€ ì¼ë°˜ ìœ ë‹ˆì½”ë“œ ì´ëª¨ì§€ (fitì—ì„œ ìì£¼ ë³´ì¸ ê²ƒë“¤)
    re.compile(r'[ğŸ‘ğŸ˜‚ğŸ˜¥ğŸ¤”]'): "[EMOJI]",
}

def replace_emotions_with_tokens(text: str) -> str:
    """
    í† í¬ë‚˜ì´ì €ì— ë„£ê¸° ì „, í…ìŠ¤íŠ¸ì˜ ê°ì„± í‘œí˜„ì„ íŠ¹ìˆ˜ í† í° ë¬¸ìì—´ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
    """
    for pattern, token in EMOTION_MAP.items():
        text = pattern.sub(token, text)
    return text

# --- 2ë‹¨ê³„: í† í¬ë‚˜ì´ì € ë¡œë“œ ë° íŠ¹ìˆ˜ í† í° ì¶”ê°€ ---

print("í† í¬ë‚˜ì´ì € ë¡œë“œ ë° íŠ¹ìˆ˜ í† í° ì¶”ê°€ ì¤‘...")
MODEL_NAME = "beomi/kcbert-base"  # ìš°ë¦¬ê°€ ì„ íƒí•œ ëª¨ë¸
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

print(f"ì›ë³¸ ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(tokenizer)}")
tokenizer.add_special_tokens({"additional_special_tokens": NEW_SPECIAL_TOKENS})
print(f"ì‹ ê·œ í† í° ì¶”ê°€ í›„ ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(tokenizer)}")

# ì¤‘ìš”: ì´ ì‘ì—… í›„, ëª¨ë¸ì„ ë¡œë“œí•  ë•Œ ë°˜ë“œì‹œ
# model.resize_token_embeddings(len(tokenizer)) ë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.

# --- 3ë‹¨ê³„: ê°œì„ ëœ BERT ì¸ì½”ë”© í•¨ìˆ˜ (ë°°ì¹˜ ì²˜ë¦¬) ---

def bert_tokenize_and_encode(texts: list, tokenizer, max_length=256, labels=None):
    """
    (ê°œì„ ) BERTë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ë°°ì¹˜(batch) í† í°í™” ë° ì¸ì½”ë”©
    - (ì‹ ê·œ) ê°ì„± í† í° ì¹˜í™˜ ê¸°ëŠ¥ í¬í•¨
    - (ê°œì„ ) for ë£¨í”„ ëŒ€ì‹  ë¹ ë¥¸ ë°°ì¹˜ ì¸ì½”ë”© ì‚¬ìš©
    """
    
    # 1. (ì‹ ê·œ) í…ìŠ¤íŠ¸ë¥¼ íŠ¹ìˆ˜ í† í° ë¬¸ìì—´ë¡œ ë¨¼ì € ì¹˜í™˜í•©ë‹ˆë‹¤.
    # (ì´ ì‘ì—…ì´ ë¹ ì§€ë©´ [LAUGH] í† í°ì´ ì•„ë¬´ ì˜ë¯¸ê°€ ì—†ìŠµë‹ˆë‹¤)
    print("ê°ì„± í‘œí˜„ì„ íŠ¹ìˆ˜ í† í° ë¬¸ìì—´ë¡œ ì¹˜í™˜ ì¤‘...")
    try:
        processed_texts = [replace_emotions_with_tokens(text) for text in texts]
    except Exception as e:
        print(f"Error during text replacement: {e}")
        processed_texts = texts # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©

    print("ë°°ì¹˜ ì¸ì½”ë”© ìˆ˜í–‰ ì¤‘ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬)...")
    # 2. (ê°œì„ ) for ë£¨í”„ ëŒ€ì‹ , ë¦¬ìŠ¤íŠ¸ ì „ì²´ë¥¼ í•œ ë²ˆì— í† í¬ë‚˜ì´ì €ì— ì „ë‹¬
    encoded_batch = tokenizer(
        processed_texts,
        add_special_tokens=True,      # [CLS], [SEP] í† í° ì¶”ê°€
        max_length=max_length,        # ìµœëŒ€ ê¸¸ì´ ì œí•œ
        padding="max_length",         # ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©
        truncation=True,              # ê¸¸ì´ ì´ˆê³¼ì‹œ ì ˆë‹¨
        return_attention_mask=True,   # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
        return_token_type_ids=True,   # í† í° íƒ€ì… ID ìƒì„± (BERT/KcBERTëŠ” ì‚¬ìš©)
        return_tensors="pt",          # PyTorch í…ì„œë¡œ ë°˜í™˜
    )
    
    # 'encoded_batch'ëŠ” ì´ë¯¸ 'input_ids', 'attention_mask' ë“±ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    
    if labels is not None:
        # labelsë¥¼ íŒŒì´í† ì¹˜ í…ì„œë¡œ ë³€í™˜
        encoded_batch["labels"] = torch.tensor(labels, dtype=torch.long)

    print("ë°°ì¹˜ ì¸ì½”ë”© ì™„ë£Œ.")
    return encoded_batch

# --- (ìˆ˜ì • ì—†ìŒ) í† í° ê¸¸ì´ ë¶„ì„ í•¨ìˆ˜ ---
def analyze_token_lengths(texts, tokenizer, max_length=256):
    """í† í° ê¸¸ì´ ë¶„í¬ ë¶„ì„ (tokenizerë¥¼ ì¸ìë¡œ ë°›ë„ë¡ ìˆ˜ì •)"""
    token_lengths = []
    
    # ê°ì„± í† í° ì¹˜í™˜ì„ ë¨¼ì € ì ìš©
    processed_texts = [replace_emotions_with_tokens(text) for text in texts]

    print("í† í° ê¸¸ì´ ë¶„ì„ ì¤‘...")
    for text in processed_texts:
        tokens = tokenizer.tokenize(text) # .tokenize()ëŠ” íŠ¹ìˆ˜í† í°([CLS]) ë¯¸í¬í•¨
        token_lengths.append(len(tokens))

    token_lengths = np.array(token_lengths)

    print(
        f"í† í° ê¸¸ì´: í‰ê·  {token_lengths.mean():.1f}, ì¤‘ì•™ê°’ {np.median(token_lengths):.1f}, ë²”ìœ„ {token_lengths.min()}-{token_lengths.max()}"
    )
    print(
        f"ë¶„ì„ ê¸°ì¤€ MaxLength {max_length} í† í° ì´ˆê³¼: {(token_lengths > max_length).sum()}ê°œ ({(token_lengths > max_length).mean() * 100:.1f}%)"
    )
    return token_lengths
```

- í† í¬ë‚˜ì´ì €ì— ì»¤ìŠ¤í…€í•˜ê¸° ìœ„í•´ íŠ¹ìˆ˜í† í°ì„ ì¶”ê°€í•œë‹¤.

```python
--- ìˆ˜ì§‘ëœ ë¼ë²¨ë³„ ìƒìœ„ ê°ì„± íŒ¨í„´ (ì˜ˆì‹œ) ---
  [Label 0]: [('ã… ã… ', 33867), ('ã…‹ã…‹', 7427), ('ã…¡ã…¡', 2363), ('ã…ã…', 954), ('ã…œã…œ', 519), ('ğŸ˜¡', 44), ('â˜…', 33), ('ã…‹ã…‹ã… ã… ', 30), ('ã…¡ã…¡ã…‹', 26), ('ã…œã… ', 24)]
  [Label 1]: [('ã… ã… ', 9489), ('ã…ã…', 2362), ('ã…‹ã…‹', 1856), ('ã…œã…œ', 86), ('ã…¡ã…¡', 78), ('â˜…', 26), ('ğŸ˜Š', 22), ('ğŸŒŸ', 18), ('ã… ã…œ', 12), ('â™¥', 11)]
  [Label 2]: [('ã…ã…', 19727), ('ã…‹ã…‹', 9407), ('ã… ã… ', 5677), ('â™¥', 832), ('ğŸŒŸ', 640), ('âœ¨', 510), ('ã…œã…œ', 477), ('ğŸ˜Š', 415), ('â™¡', 360), ('ğŸ‘', 351)]
  [Label 3]: [('ã…ã…', 5713), ('ã… ã… ', 4948), ('ã…‹ã…‹', 3900), ('â™¥', 2067), ('â™¡', 571), ('ã…œã…œ', 391), ('ğŸŒŸ', 330), ('â˜…', 206), ('âœ¨', 142), ('ğŸ‘', 137)]
------------------------------------------
```

ìˆ˜ì§‘ëœ ë¼ë²¨ë³„ ê°ì„±íŒ¨í„´ì„ ë³´ë©´ â€˜ã… ã… â€™ë‚˜ â€˜ã…‹ã…‹â€™ê°™ì€ ê°ì •íŒ¨í„´ì€ ë ˆì´ë°œê³¼ ìƒê´€ì—†ì´ ìì£¼ ë“±ì¥í•¨ì„ ë³¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë…¸ì´ì¦ˆì²˜ë¦¬ë¥¼ í•´ì•¼í•˜ë‚˜ ê³ ë¯¼ì„ í•´ë³´ì•˜ë‹¤.

í•˜ì§€ë§Œ BERTê¸°ë°˜ ëª¨ë¸ì€ ë¬¸ë§¥ì„ ì´í•´í•˜ë¯€ë¡œ ì´ëŸ¬í•œ í† í°ì„ ë‚¨ê²¨ë‘ëŠ”ê²Œ ìœ ë¦¬í•˜ë‹¤.

- Bidirectional Processing: BERTëŠ” í•œë°©í–¥ìœ¼ë¡œë§Œ í•™ìŠµì„ í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ë¬¸ì¥ ì „ì²´ë¥¼ í•œë²ˆì— ì–´í…ì…˜ìœ¼ë¡œ ë³´ê³ , íŠ¹ì • ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í• ë•Œ ê·¸ ë‹¨ì–´ì˜ ì•ë’¤ ëª¨ë“  ë‹¨ì–´ì •ë³´ë¥¼ ë™ì‹œì— í™œìš©í•œë‹¤.
- Self-Attention: BERTëŠ” Multi Head-self Attention(headëŠ” 12ê°œ)ì„ ì´ìš©í•´ì„œ ë¬¸ì¥ ì „ì²´ì˜ ë¬¸ë§¥ì„ ë³‘ë ¬ì ìœ¼ë¡œ íŒŒì•…í•œë‹¤.

# ë°ì´í„° ë¶„í•  ì „ëµ

- í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• 
- í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ìœ ì§€í•˜ëŠ” ê³„ì¸µì  ë¶„í• 

ìš°ì„  stratified K-Fold cross validationì„ ì‚¬ìš©í•œë‹¤.  ë°ì´í„°ì…‹ì„ 5ê°œë¡œ ë‚˜ëˆ„ê³ , ê°ê° validationë°ì´í„°ë¥¼ 20%, train ë°ì´í„°ë¥¼ 80%ë¡œ ì‚¬ìš©í•´ì„œ ì„œë¡œ ë‹¤ë¥¸ ê²½ìš°ì˜ìˆ˜ 5ë²ˆì„ í•™ìŠµì‹œí‚¨ë‹¤. ì´í›„ ì¶”ë¡ ë‹¨ê³„ì—ì„œëŠ” 5ê°œì˜ ëª¨ë¸ì„ ì•™ìƒë¸”í•´ì„œ ì†Œí”„íŠ¸ votingì„ í†µí•´ ì¶”ë¡ ê²°ê³¼ë¥¼ ë„ì¶œí•´ë‚¸ë‹¤.

```python
from sklearn.model_selection import StratifiedKFold
from sklearn.utils.class_weight import compute_class_weight
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, DataCollatorWithPadding, Trainer
import numpy as np
import pandas as pd
import torch
import os
import torch.nn.functional as F # ì•™ìƒë¸” ì‹œ í•„ìš”
from torch.utils.data import DataLoader # ì¶”ë¡  ì‹œ í•„ìš”

# (ê¸°ì¡´ ë³€ìˆ˜ë“¤: USE_WANDB, SAVE_MODEL, df, RANDOM_STATE, ...)
# (ê¸°ì¡´ í´ë˜ìŠ¤/í•¨ìˆ˜ ì •ì˜: TextPreprocessingPipeline, replace_emotions_with_tokens, ReviewDataset, CustomTrainer, compute_metrics, ...)

USE_WANDB = True
SAVE_MODEL = True # â˜…â˜…â˜… ëª¨ë¸ ì €ì¥ì„ Trueë¡œ ìœ ì§€

# ì‚¬ìš©í•  ì „ì²´ ë°ì´í„° (ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© ì „ ì›ë³¸)
X_full = df["review"]
y_full = df["label"]
ids_full = df["ID"]

# Stratified K-Fold ì„¤ì •
N_SPLITS = 5
skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)

print(f"\nStratified {N_SPLITS}-Fold Cross Validation ì‹œì‘...")

fold_metrics = []
all_fold_train_history = []
saved_model_paths = [] # â˜…â˜…â˜… [ì¶”ê°€] ê° Foldì˜ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

# K-Fold ë£¨í”„ ì‹œì‘
for fold, (train_index, val_index) in enumerate(skf.split(X_full, y_full)):
    print(f"\n===== Fold {fold+1}/{N_SPLITS} =====")

    # ... (1. ë°ì´í„° ë¶„í•  ~ 7. ë°ì´í„°ì…‹ ìƒì„± ê¹Œì§€ëŠ” ë™ì¼) ...
    
    # 1. í˜„ì¬ Foldì˜ í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í•  (ì›ë³¸ ë°ì´í„° ì‚¬ìš©)
    X_train, X_val = X_full.iloc[train_index], X_full.iloc[val_index]
    y_train, y_val = y_full.iloc[train_index], y_full.iloc[val_index]
    ids_train, ids_val = ids_full.iloc[train_index], ids_full.iloc[val_index]
    print(f"í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(X_val)}ê°œ")

    # 2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© (ê° Foldë§ˆë‹¤ ìƒˆë¡œ í•™ìŠµ)
    preprocessor_fold = TextPreprocessingPipeline() # ë§¤ Foldë§ˆë‹¤ ìƒˆ ì¸ìŠ¤í„´ìŠ¤
    print("í˜„ì¬ Fold í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í•™ìŠµ ë° ì ìš©...")
    X_train_processed = preprocessor_fold.fit_transform(X_train, y_train)
    print("í˜„ì¬ Fold ê²€ì¦ ë°ì´í„°ì— ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©...")
    X_val_processed = preprocessor_fold.transform(X_val)

    # 3. íŠ¹ìˆ˜ í† í° ì¹˜í™˜ ì ìš©
    print("í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì— íŠ¹ìˆ˜ í† í° ì¹˜í™˜ ì ìš© ì¤‘...")
    X_train_replaced = [replace_emotions_with_tokens(text) for text in X_train_processed.tolist()]
    X_val_replaced = [replace_emotions_with_tokens(text) for text in X_val_processed.tolist()]
    print("âœ“ íŠ¹ìˆ˜ í† í° ì¹˜í™˜ ì™„ë£Œ")

    # 4. ëª¨ë¸ í•™ìŠµìš© DataFrame ìƒì„± (í˜„ì¬ Fold ìš©)
    train_data_fold = pd.DataFrame(
        {"ID": ids_train.tolist(), "review": X_train_replaced, "label": y_train.tolist()}
    )
    val_data_fold = pd.DataFrame(
        {"ID": ids_val.tolist(), "review": X_val_replaced, "label": y_val.tolist()}
    )

    # 5. í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ë° íŠ¹ìˆ˜ í† í° ì¶”ê°€ (ë§¤ Foldë§ˆë‹¤)
    print(f"\nFold {fold+1}: í† í¬ë‚˜ì´ì € ë¡œë”© ë° ìˆ˜ì • ({model_name})")
    tokenizer_fold = AutoTokenizer.from_pretrained(model_name)
    tokenizer_fold.add_special_tokens({"additional_special_tokens": NEW_SPECIAL_TOKENS})
    print(f"Fold {fold+1}: ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(tokenizer_fold)}")

    # 6. ëª¨ë¸ ì´ˆê¸°í™” ë° ì„ë² ë”© í¬ê¸° ì¡°ì • (ë§¤ Foldë§ˆë‹¤)
    print(f"Fold {fold+1}: ëª¨ë¸ ë¡œë”© ë° ìˆ˜ì • ({model_name})")
    model_fold = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=NUM_CLASSES,
    )
    model_fold.resize_token_embeddings(len(tokenizer_fold))
    print(f"Fold {fold+1}: ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì • ì™„ë£Œ.")
    model_fold.to(device) # ëª¨ë¸ì„ GPUë¡œ ì´ë™

    # 7. ë°ì´í„°ì…‹ ìƒì„± (í˜„ì¬ Fold ë°ì´í„° ì‚¬ìš©)
    print(f"Fold {fold+1}: ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    train_dataset_fold = ReviewDataset(
        train_data_fold["review"],
        train_data_fold["label"],
        tokenizer_fold,
        CHOSEN_MAX_LENGTH,
    )
    val_dataset_fold = ReviewDataset(
        val_data_fold["review"],
        val_data_fold["label"],
        tokenizer_fold,
        CHOSEN_MAX_LENGTH,
    )
    print(f"Fold {fold+1}: í›ˆë ¨ {len(train_dataset_fold)}ê°œ, ê²€ì¦ {len(val_dataset_fold)}ê°œ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ.")
    
    # 8. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (í˜„ì¬ Fold í›ˆë ¨ ë°ì´í„° ê¸°ì¤€)
    print(f"Fold {fold+1}: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")
    class_weights_fold = compute_class_weight(
        'balanced',
        classes=np.unique(y_train), # í˜„ì¬ Foldì˜ y_train ì‚¬ìš©
        y=y_train.to_numpy()
    )
    class_weights_tensor_fold = torch.tensor(class_weights_fold, dtype=torch.float)
    print(f"Fold {fold+1}: ê³„ì‚°ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weights_tensor_fold}")

    # 9. TrainingArguments ì„¤ì • (ì¶œë ¥ ë””ë ‰í† ë¦¬ ë³€ê²½)
    # â˜…â˜…â˜… output_dirì„ ëª…í™•í•˜ê²Œ ì§€ì •í•©ë‹ˆë‹¤.
    output_dir_fold = f"./results_fold_{fold+1}"
    
    training_args_fold = TrainingArguments(
        output_dir=output_dir_fold, # Foldë³„ ë””ë ‰í† ë¦¬
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE_TRAIN,
        per_device_eval_batch_size=BATCH_SIZE_EVAL,
        warmup_steps=WARMUP_STEPS,
        weight_decay=WEIGHT_DECAY,
        learning_rate=LEARNING_RATE,
        logging_steps=100,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True, # â˜…â˜…â˜… í›ˆë ¨ ì¢…ë£Œ í›„ ìµœê³  ëª¨ë¸ ë¡œë“œ
        metric_for_best_model="accuracy", 
        greater_is_better=True,
        save_total_limit=1,
        report_to="wandb" if USE_WANDB else "none",
        run_name=f"kfold_{fold+1}_{model_name.split('/')[-1]}" if USE_WANDB else None,
        seed=RANDOM_STATE,
        fp16=torch.cuda.is_available(),
        dataloader_num_workers=2,
        remove_unused_columns=False,
    )

    # 10. Custom Trainer ì •ì˜ (ì´ë¯¸ ì™„ë£Œë¨)

    # 11. Custom Trainer ì´ˆê¸°í™” (í˜„ì¬ Fold ë°ì´í„° ë° ê°€ì¤‘ì¹˜ ì‚¬ìš©)
    print(f"Fold {fold+1}: Custom Trainer ì´ˆê¸°í™” ì¤‘...")
    trainer_fold = CustomTrainer(
        model=model_fold,
        args=training_args_fold,
        train_dataset=train_dataset_fold,
        eval_dataset=val_dataset_fold,
        tokenizer=tokenizer_fold,
        data_collator=DataCollatorWithPadding(tokenizer=tokenizer_fold),
        compute_metrics=compute_metrics,
        class_weights=class_weights_tensor_fold
    )
    print(f"Fold {fold+1}: âœ“ Custom Trainer ì´ˆê¸°í™” ì™„ë£Œ")

    # 12. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
    print(f"Fold {fold+1}: ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    try:
        train_result = trainer_fold.train()
        print(f"Fold {fold+1}: âœ“ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")

        # 13. í˜„ì¬ Fold ê²€ì¦ ì„±ëŠ¥ í‰ê°€ ë° ì €ì¥
        print(f"Fold {fold+1}: ê²€ì¦ ë°ì´í„° í‰ê°€ ì¤‘...")
        eval_results = trainer_fold.evaluate()
        fold_metrics.append(eval_results) # ê²°ê³¼ ì €ì¥
        print(f"Fold {fold+1}: âœ“ ê²€ì¦ ì™„ë£Œ: {eval_results}")

        # --- â˜…â˜…â˜… [ìˆ˜ì •] ì•™ìƒë¸”ì„ ìœ„í•œ ëª¨ë¸/í† í¬ë‚˜ì´ì € ì €ì¥ ---
        # load_best_model_at_end=Trueì´ë¯€ë¡œ,
        # trainer_fold.modelì€ ì´ë¯¸ ë² ìŠ¤íŠ¸ ëª¨ë¸ ìƒíƒœì…ë‹ˆë‹¤.
        if SAVE_MODEL:
            # TrainingArgumentsì˜ output_dirê³¼ ë³„ê°œë¡œ
            # ì•™ìƒë¸”ì— ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ëª…í™•í•œ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.
            save_path = f"./best_model_fold_{fold+1}"
            trainer_fold.save_model(save_path)
            tokenizer_fold.save_pretrained(save_path)
            
            # [ì¶”ê°€] ì €ì¥ëœ ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            saved_model_paths.append(save_path) 
            print(f"Fold {fold+1}: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        # ---------------------------------------------------

    except Exception as e:
        print(f"Fold {fold+1}: í›ˆë ¨/í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        fold_metrics.append(None)

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model_fold, tokenizer_fold, train_dataset_fold, val_dataset_fold, trainer_fold
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# K-Fold ë£¨í”„ ì¢…ë£Œ
print(f"\n===== Stratified {N_SPLITS}-Fold Cross Validation ì¢…ë£Œ =====")

# --- ìµœì¢… ê²°ê³¼ ë¶„ì„ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
# ... (avg_accuracy, std_accuracy ë“± ê³„ì‚° ë° ì¶œë ¥) ...
print("\n===== ìµœì¢… êµì°¨ ê²€ì¦ ê²°ê³¼ ë¶„ì„ =====")
valid_results = [m for m in fold_metrics if m is not None]
if len(valid_results) > 0:
    avg_accuracy = np.mean([m['eval_accuracy'] for m in valid_results])
    std_accuracy = np.std([m['eval_accuracy'] for m in valid_results])
    # ... (ê¸°íƒ€ F1, Loss ê³„ì‚°) ...
    print(f"ì´ {len(valid_results)}ê°œ Fold ê²°ê³¼ ë¶„ì„:")
    print(f"  í‰ê·  ê²€ì¦ ì •í™•ë„ (Accuracy): {avg_accuracy:.4f} Â± {std_accuracy:.4f}")
    # ... (ê¸°íƒ€ F1, Loss ì¶œë ¥) ...
else:
    print("ì˜¤ë¥˜ë¡œ ì¸í•´ ìœ íš¨í•œ êµì°¨ ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
print("===================================")
```

![image](/assets/images/2025-10-25-18-38-38.png)

ë¬´ì–¸ê°€ ì˜ëª»ëœê²ƒì¸ì§€ ê° Foldë§ˆë‹¤ Training LossëŠ” ì¤„ì–´ë“œëŠ” ì¶”ì„¸ë¥¼ ë³´ì´ì§€ë§Œ, Validation LossëŠ” ì¤„ì–´ë“¤ì§€ ì•ŠëŠ” ê²½í–¥ì„ ë³´ì˜€ë‹¤. ê·¸ë¦¬ê³  Training LossëŠ” ê³„ì†í•´ì„œ ì¤„ì–´ë“œëŠ” ì¶”ì„¸ë¥¼ ë³´ì—¬ì„œ, ì´í›„ì—ëŠ” ê³¼ì í•© ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤ìŒ, epochì„ ë” ëŠ˜ë ¤ì„œ Lossë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì„ ê³ ë ¤í•´ë´ì•¼ê² ë‹¤.

ìµœì¢… Cross Validationì„í‰ê· ë‚´ë³´ë©´ 0.8318ë¡œ ì´ˆê¸° ëª¨ë¸ì˜ accì¸ 0.8ë³´ë‹¤ 3%ì •ë„ ì„±ëŠ¥í–¥ìƒì´ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
print("\n===== ì•™ìƒë¸” ì¶”ë¡ (Inference) ì‹œì‘ =====")

# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (e.g., Cell 37)
print("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©...")
try:
    # (test.csv ê²½ë¡œëŠ” ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
    df_test = pd.read_csv("test.csv") 
    X_test = df_test["review"]
    ids_test = df_test["ID"]
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° {len(df_test)}ê°œ ë¡œë“œ ì™„ë£Œ.")
except FileNotFoundError:
    print("Error: test.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¶”ë¡ ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    # ì´ ê²½ìš° ì•„ë˜ ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

if 'df_test' in locals(): # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    
    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬
    # â˜…â˜…â˜… ì¤‘ìš”: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì€ K-Foldì— ì‚¬ìš©í•œ *ì „ì²´* í›ˆë ¨ ë°ì´í„°ë¡œ
    # ë‹¤ì‹œ í•™ìŠµ(fit)í•œ í›„, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©(transform)í•´ì•¼ í•©ë‹ˆë‹¤.
    # (Kê°œ Fold ì¤‘ í•˜ë‚˜ì˜ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ í¸í–¥ë  ìˆ˜ ìˆìŒ)
    
    print("\n2. ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ìµœì¢… ì „ì²˜ë¦¬ê¸° í•™ìŠµ...")
    final_preprocessor = TextPreprocessingPipeline()
    # K-Fold ë£¨í”„ ë°–ì˜ X_full, y_full ì‚¬ìš©
    final_preprocessor.fit(X_full, y_full) 
    
    print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì „ì²˜ë¦¬ê¸° ì ìš©...")
    X_test_processed = final_preprocessor.transform(X_test)
    
    print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— íŠ¹ìˆ˜ í† í° ì¹˜í™˜ ì ìš©...")
    X_test_replaced = [replace_emotions_with_tokens(text) for text in X_test_processed.tolist()]
    print("âœ“ ì „ì²˜ë¦¬ ë° ì¹˜í™˜ ì™„ë£Œ.")

    # 3. K-Fold ëª¨ë¸ ì•™ìƒë¸” ì¶”ë¡ 
    all_fold_logits = [] # Kê°œ ëª¨ë¸ì˜ ë¡œì§“(logits)ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    saved_model_paths = [f"./best_model_fold_{i+1}" for i in range(N_SPLITS)]
    print(f"\n3. {len(saved_model_paths)}ê°œ Fold ëª¨ë¸ë¡œ ì¶”ë¡  ìˆ˜í–‰...")

    for fold, model_path in enumerate(saved_model_paths):
        print(f"--- Fold {fold+1} ëª¨ë¸ ì¶”ë¡  ({model_path}) ---")
        
        # A. Foldë³„ ì €ì¥ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©...")
        tokenizer_inf = AutoTokenizer.from_pretrained(model_path)
        model_inf = AutoModelForSequenceClassification.from_pretrained(model_path)
        model_inf.to(device)
        model_inf.eval()
        
        # B. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± (Foldì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©)
        # ReviewDatasetì´ ë ˆì´ë¸”ì„ í•„ìš”ë¡œ í•˜ë¯€ë¡œ, ë”ë¯¸ ë ˆì´ë¸” ìƒì„±
        test_labels_dummy = [0] * len(X_test_replaced)
        
        test_dataset_inf = ReviewDataset(
            reviews=X_test_replaced,
            labels=test_labels_dummy,
            tokenizer=tokenizer_inf,
            max_length=CHOSEN_MAX_LENGTH
        )
        
        test_data_collator = DataCollatorWithPadding(tokenizer=tokenizer_inf)
        
        # C. ì¶”ë¡  ì‹¤í–‰ (Trainerì˜ .predict() ë©”ì†Œë“œ í™œìš©)
        # ì¶”ë¡ ìš© TrainingArguments (ê°„ë‹¨í•˜ê²Œ ì„¤ì •)
        test_args = TrainingArguments(
            output_dir=f"./temp_inference_output_{fold+1}",
            per_device_eval_batch_size=BATCH_SIZE_EVAL, # í›ˆë ¨ ì‹œ eval ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
            fp16=torch.cuda.is_available(),
            dataloader_num_workers=2,
            report_to="none", # ì¶”ë¡  ì‹œ ë¡œê·¸ ë¶ˆí•„ìš”
        )

        trainer_inf = Trainer(
            model=model_inf,
            args=test_args,
            tokenizer=tokenizer_inf,
            data_collator=test_data_collator,
        )
        
        # .predict()ëŠ” (predictions, label_ids, metrics) íŠœí”Œ ë°˜í™˜
        # predictionsëŠ” ë¡œì§“(logits)ì´ ë‹´ê¸´ numpy ë°°ì—´ì…ë‹ˆë‹¤.
        predictions_output = trainer_inf.predict(test_dataset_inf)
        fold_logits_np = predictions_output.predictions
        
        # numpy ë°°ì—´ì„ torch í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        all_fold_logits.append(torch.tensor(fold_logits_np))
        print(f"Fold {fold+1} ì¶”ë¡  ì™„ë£Œ. ë¡œì§“ shape: {fold_logits_np.shape}")

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model_inf, tokenizer_inf, test_dataset_inf, trainer_inf
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # 4. K-Fold ì˜ˆì¸¡ ê²°ê³¼ ì•™ìƒë¸” (Averaging)
    print("\n4. K-Fold ì˜ˆì¸¡ ê²°ê³¼ ì•™ìƒë¸” (Averaging)...")
    
    # all_fold_logits ë¦¬ìŠ¤íŠ¸ì—ëŠ” (num_test_samples, num_classes) í…ì„œê°€ Kê°œ ìˆìŒ
    # torch.stackì„ ì‚¬ìš©í•´ (K, num_test_samples, num_classes) í…ì„œë¡œ ë§Œë“¦
    stacked_logits = torch.stack(all_fold_logits)
    print(f"Stacked logits shape (K, N_samples, N_classes): {stacked_logits.shape}")
    
    # Kê°œ ëª¨ë¸ì˜ ë¡œì§“ì„ í‰ê·  (dim=0 : K(fold) ì°¨ì›)
    mean_logits = torch.mean(stacked_logits, dim=0)
    print(f"Mean logits shape (N_samples, N_classes): {mean_logits.shape}")

    # ìµœì¢… ì˜ˆì¸¡ í´ë˜ìŠ¤ (ê°€ì¥ ë†’ì€ ë¡œì§“ì˜ ì¸ë±ìŠ¤)
    final_predictions = torch.argmax(mean_logits, dim=1)
    
    # (ì°¸ê³ ) ìµœì¢… í™•ë¥ ê°’ì´ í•„ìš”í•œ ê²½ìš°
    # final_probs = F.softmax(mean_logits, dim=1)

    # 5. ì œì¶œ íŒŒì¼ ìƒì„±
    print("\n5. ì œì¶œ íŒŒì¼ ìƒì„±...")
    submission_df = pd.DataFrame({
        'ID': ids_test, # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ID ì‚¬ìš©
        'label': final_predictions.numpy() # í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
    })

    submission_filename = "ensemble_submission.csv"
    submission_df.to_csv(submission_filename, index=False)
    print(f"âœ“ ì œì¶œ íŒŒì¼ '{submission_filename}' ìƒì„± ì™„ë£Œ!")
    print(submission_df.head())

else:
    print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ì¶”ë¡ ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

print("===================================")
```

ê° Foldë§ˆë‹¤ ë‹¤ë¥¸ ëª¨ë¸ì„ ì¨ì„œ ì•™ìƒë¸”í•˜ëŠ” ë°©ë²•ë„ ìˆê² ì§€ë§Œ, ìš°ì„  `beomi/bert` ëª¨ë¸ë§Œì„ ì‚¬ìš©í•´ì„œ, Stratified k-Fold cross Validationì„ í•´ì£¼ì—ˆë‹¤. 

ì¶”ë¡ ì„ í• ë•Œ kê°œ Foldì¤‘ í•˜ë‚˜ì˜ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•´ì„œ dictë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ í•˜ë©´ í¸í–¥ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‹¤ì‹œ ì „ì²´ ë°ì´í„°ì…‹ì„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ `fit` í•œí›„ì— ì‚¬ìš©í•´ì£¼ì–´ì•¼ í•œë‹¤.

# í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬

- Weighted Cross Entropy

```python
from sklearn.utils.class_weight import compute_class_weight
# í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° 
print("\ní´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")
# y_trainì€ train_test_splitì—ì„œ ë°˜í™˜ëœ pandas Series
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train.to_numpy() # Seriesë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
)

# PyTorch í…ì„œë¡œ ë³€í™˜
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)

print(f"ê³„ì‚°ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weights_tensor}")
```

EDAì—ì„œ í™•ì¸í–ˆë“¯ì´ ë°ì´í„°ì—ì„œ í´ë˜ìŠ¤ë³„ ë¶ˆê· í˜•ì´ ìˆì—ˆë‹¤. ë”°ë¼ì„œ í´ë˜ìŠ¤ë¹„ìœ¨ì˜ ì—­ìˆ˜ë¥¼ CrossEntropy Lossfunctionì˜ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•´ì„œ ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ LossëŠ” ê·¸ ë¹„ìœ¨ì˜ ì—­ìˆ˜ë§Œí¼ ë” ë§ì´ ë°˜ì˜ë˜ê²Œ ì²˜ë¦¬ë¥¼ í•´ì£¼ì—ˆë‹¤.

# ê²°ê³¼ ë° ì´í›„ ì‹¤í—˜ê³„íš

![image](/assets/images/2025-10-25-18-38-49.png)

ë² ì´ìŠ¤ ëª¨ë¸ì˜ ì„±ëŠ¥

![image](/assets/images/2025-10-25-18-38-54.png)

ë² ì´ìŠ¤ëª¨ë¸ì¸ ë§ˆë¼íƒ•í›„ë£¨í›„ë£¨ì— ë¹„í•´ Feature Engineeringì„ í•œ ëª¨ë¸ì´ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë–¨ì–´ì¡ŒìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì™œì¼ê¹Œâ€¦

### Weighted Cross Entropy

Weighted Cross EntropyëŠ” ì†Œìˆ˜ í´ë˜ìŠ¤ì— ëŒ€í•œ recallì„ í–¥ìƒì‹œì¼œì£¼ì§€ë§Œ, ëŒ€ë‹¤ìˆ˜ë¥¼ ì°¨ì§€í•˜ëŠ” í´ë˜ìŠ¤ì— ëŒ€í•´ì„œëŠ” ì •í™•ë„ê°€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆë‹¤. ì´ ëŒ€íšŒì˜ metricì€ accuracy í•˜ë‚˜ì´ë¯€ë¡œ ì–´ì©Œë©´ weighted cross entropyë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ê²Œ metricì¸ accuracyë¥¼ ë†’ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì¸ì§€ ëª¨ë¥´ê² ë‹¤.

### ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €ì— íŠ¹ìˆ˜í† í° ì¶”ê°€í•˜ê¸°

ê¸°ì¡´ì—ëŠ” ë­‰ëš±ê·¸ë ¤ì„œ [emoji]í† í°ì„ ë§¤í•‘í•´ì„œ ì´ëª¨ì§€ë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ìˆì—ˆë‹¤. ì´ëª¨ì§€ì—ëŠ” í™”ë‚œ ì´ëª¨í‹°ì½˜, ì›ƒëŠ”ì´ëª¨í‹°ì½˜ë“± ë‹¤ì–‘í•˜ë‹¤. ì´ë“¤ì„ êµ¬í˜„í•˜ì§€ ì•Šì•„ì„œ ì„±ëŠ¥ì´ ì˜ ì•ˆë‚˜ì˜¨ ê²ƒ ê°™ë‹¤.

```python
--- ìˆ˜ì§‘ëœ ë¼ë²¨ë³„ ìƒìœ„ ê°ì„± íŒ¨í„´ (ì˜ˆì‹œ) ---
  [Label 0]: [('ã… ã… ', 33867), ('ã…‹ã…‹', 7427), ('ã…¡ã…¡', 2363), ('ã…ã…', 954), ('ã…œã…œ', 519), ('ğŸ˜¡', 44), ('â˜…', 33), ('ã…‹ã…‹ã… ã… ', 30), ('ã…¡ã…¡ã…‹', 26), ('ã…œã… ', 24)]
  [Label 1]: [('ã… ã… ', 9489), ('ã…ã…', 2362), ('ã…‹ã…‹', 1856), ('ã…œã…œ', 86), ('ã…¡ã…¡', 78), ('â˜…', 26), ('ğŸ˜Š', 22), ('ğŸŒŸ', 18), ('ã… ã…œ', 12), ('â™¥', 11)]
  [Label 2]: [('ã…ã…', 19727), ('ã…‹ã…‹', 9407), ('ã… ã… ', 5677), ('â™¥', 832), ('ğŸŒŸ', 640), ('âœ¨', 510), ('ã…œã…œ', 477), ('ğŸ˜Š', 415), ('â™¡', 360), ('ğŸ‘', 351)]
  [Label 3]: [('ã…ã…', 5713), ('ã… ã… ', 4948), ('ã…‹ã…‹', 3900), ('â™¥', 2067), ('â™¡', 571), ('ã…œã…œ', 391), ('ğŸŒŸ', 330), ('â˜…', 206), ('âœ¨', 142), ('ğŸ‘', 137)]
------------------------------------------
```

íŠ¹íˆ ê°ì„±íŒ¨í„´ì„ ì¶”ì¶œí•´ì„œ í† í°í™”í–ˆëŠ”ë°, ë ˆì´ë¸”ê°„ êµ¬ë¶„ì—†ì´ â€˜ã…ã…â€™,â€™ã… ã… â€™,â€™ã…‹ã…‹â€™ë“±ì€ ëª¨ë‘ ìì£¼ ë“±ì¥í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ê°ì„± íŒ¨í„´ë“¤ì„ ì°¨ë¼ë¦¬ ë…¸ì´ì¦ˆë¡œ êµ¬ë¶„í•˜ëŠ”ê²Œ ì¢‹ì„ì§€ ì•„ë‹ˆë©´ íŠ¹ìˆ˜í† í°ì²˜ë¦¬í•˜ëŠ”ê²Œ ì¢‹ì„ì§€, ë‚˜ë¨¸ì§€ ë³€ì¸ë“¤ì„ í†µì œë³€ì¸ ì²˜ë¦¬í•˜ê³  ì‹¤í—˜ì„ í†µí•´ì„œ ë‹µì„ ì°¾ì•„ë´ì•¼ê² ë‹¤.

ì¶”ê°€ë¡œ ë¶ˆìš©ì–´ ì²˜ë¦¬ë¥¼ BaseModelì—ì„œëŠ” ì˜ë¯¸ê°€ ìˆëŠ”ê²ƒ ê°™ì€ ë‹¨ì–´ë“¤ë„ ëª¨ë‘ ì²˜ë¦¬ë¥¼ í–ˆì—ˆëŠ”ë°, ìˆ˜ì •í•œ ëª¨ë¸ì—ì„œëŠ” ë¶ˆìš©ì–´ë¥¼ ê¸°ë³¸ì ì¸ ì¡°ì‚¬, ì–´ë¯¸ë¡œ ëŒ€í­ ì¤„ì˜€ë‹¤. ì¦‰ â€˜ì§„ì§œâ€™ê°™ì€ ë‹¨ì–´ë“¤ì€ ë¶ˆìš©ì–´ì²˜ë¦¬ê°€ ë˜ì–´ìˆì—ˆëŠ”ë°, â€˜ì§„ì§œâ€™ë¼ëŠ” ë‹¨ì–´ëŠ” ì˜ë¯¸ë¥¼ ë‹´ê³ ìˆì§€ë§Œ, ë ˆì´ë¸”ì— ìƒê´€ì—†ì´ ëª¨ë“ ë¬¸ì¥ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì´ë‹¤. ë”°ë¼ì„œ ë¶ˆìš©ì–´ë¥¼ í•´ì œí•˜ë‹ˆê¹, ì„±ëŠ¥ì´ í•˜ë½í•œê²Œ ì•„ë‹Œê°€ ì‹¶ë‹¤. ì´ê²ƒë„ ë‚˜ë¨¸ì§€ ë³€ì¸ë“¤ì„ í†µì œë³€ì¸ ì²˜ë¦¬í•˜ê³  ì‹¤í—˜ì„ í†µí•´ì„œ ë¶ˆìš©ì–´ì²˜ë¦¬ë¥¼ ì–´ë–»ê²Œ í•´ë‚˜ê°ˆì§€ ë‹µì„ ì°¾ì•„ë‚˜ê°€ë´ì•¼ê² ë‹¤.

í•œë²ˆì— ì—¬ëŸ¬ê°€ì§€ ê¸°ëŠ¥ì„ êµ¬í˜„í•´ë³´ë‹ˆê¹, ì–´ë–¤ ìš”ì¸ì´ ì„±ëŠ¥ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì³¤ëŠ”ì§€ ë¹„êµí•˜ê¸°ê°€ ì–´ë ¤ì› ë‹¤. ë‚˜ë¨¸ì§€ ë³€ì¸ë“¤ì„ í†µì œë³€ì¸ìœ¼ë¡œ ì„¤ì •í•˜ê³ , í•œê°€ì§€ì”© ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œì— ì ìš©í•´ë³´ë©´ì„œ ì‹¤í—˜ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‚˜ì¤‘ì— í•œë²ˆì— ëª¨ë¸ë¡œ êµ¬í˜„í•´ë´ì•¼ê² ë‹¤.