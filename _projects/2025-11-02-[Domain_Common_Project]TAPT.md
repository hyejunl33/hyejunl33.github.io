---
layout: single
title: "[ëª¨ë¸ìµœì í™”]ë§ˆì§€ë§‰ ì‹œë„, ê·¸ë¦¬ê³  ë°˜ì„±"
date: 2025-11-01
tags:
  - Domain_Common_Project
  - study
  - ModelOptimization
excerpt: "[ëª¨ë¸ìµœì í™”]TAPT, WeightedModelEnsemble"
math: true
---



# Introduction

- taptì ìš©
- ì „ì²´ë°ì´í„°ì— ëŒ€í•´ì„œ 5ê°œëª¨ë¸ ë‹¤ì‹œ í•™ìŠµ í›„ ì•™ìƒë¸” â†’ ì‹œê°„ë¶€ì¡±ìœ¼ë¡œ ëª»í•´ë´„
- ì „ì²´ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ ëª¨ë¸ íš¨ê³¼ìˆëŠ”ì§€ ì‹¤í—˜â†’ ì‹œê°„ë¶€ì¡±ìœ¼ë¡œ ëª»í•´ë´„
- 5ê°œëª¨ë¸ accë¥¼ Weightë¡œ ì¤˜ì„œ ì•™ìƒë¸”
- ì—­ë²ˆì—­ì¦ê°• ì‚¬ìš©í• ì§€ ë§ì§€ ê²°ì • â†’ ë²ˆì—­ì˜ í’ˆì§ˆì´ ì•ˆì¢‹ì•„ì„œ ê²°êµ­ì—” ì‚¬ìš© x

ì €ë²ˆì‹¤í—˜ê¹Œì§€ í•´ì„œ ê¸°ê» í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•´ë’€ëŠ”ë° TAPTë¼ëŠ” ìƒˆë¡œìš´ ê¸°ë²•ì— ëŒ€í•´ ì•Œê²Œëë‹¤. ì¼ë°˜ì ìœ¼ë¡œ BERTëª¨ë¸ì„ ë‚´ê°€ ì›í•˜ëŠ” TASKì— FineTuningí•˜ëŠ” ê¸°ë²•ì¸ë°, ì´ ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ ì¼ë°˜ì ìœ¼ë¡œ í–¥ìƒëœë‹¤ê³  í•œë‹¤. ì´ ê¸°ë²•ì„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•œ í›„ì— ì•Œê²Œë˜ì–´ì„œ, ëŒ€íšŒ ì¢…ë£Œê¹Œì§€ 2ì¼ ë‚¨ì€ ì‹œì ì—ì„œ, ë‹¤ì‹œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•´ì•¼í•˜ëŠ” ìƒí™©ì— ì²˜í•´ì¡Œë‹¤. ë”°ë¼ì„œ TAPTë¥¼ ì¼ë‹¨ ì ìš©í•´ë³´ê³ , ê¸°ì¡´ì— ì°¾ì•˜ë˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ì„œ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•´ë³´ê¸°ë¡œ í–ˆë‹¤.

[TAPTì •ë¦¬ê¸€]([https://hyejunl33.github.io/projects/2025-11-01-[Domain_Common_Project]TAPT/](https://hyejunl33.github.io/projects/2025-11-01-%5BDomain_Common_Project%5DTAPT/))

ê·¸ ì™¸ì—ë„ ëª¨ë¸ë§ˆë‹¤ accì„±ëŠ¥ì´ ë‹¤ë¥´ë¯€ë¡œ, ì¶”ë¡ ë‹¨ê³„ì—ì„œ ëª¨ë¸ì„ ì•™ìƒë¸” í• ë•Œ ê·¸ëƒ¥ logitì„ softvotingí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ëª¨ë¸ê°„ì˜ accë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì¤˜ì„œ softvotingí•˜ëŠ” ë°©ë²•ì„ êµ¬í˜„í–ˆë‹¤.

ì—­ë²ˆì—­ ì¦ê°•ì€ ì½”ë“œê¹Œì§€ëŠ” êµ¬í˜„í–ˆìœ¼ë‚˜, ê°•í•œë¶€ì •, ê°•í•œê¸ì •ì—ì„œëŠ” ì–´ëŠì •ë„ ë¬¸ë§¥ì´ ìœ ì§€ë˜ëŠ”ë“¯ í–ˆëŠ”ë°, ì•½í•œê¸ì •, ì•½í•œë¶€ì •ì„ ì—­ë²ˆì—­í•˜ë‹ˆ, ë¬¸ë§¥ì´ ì•„ì˜ˆ ë‹¤ë¥¸ ë°ì´í„°ë“¤ì´ ìƒì„±ë˜ëŠ”ê²ƒì„ í™•ì¸í–ˆë‹¤. ë” ì •êµí•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´, ì—­ë²ˆì—­ ì¦ê°•ì„ ì‚¬ìš©í•´ì„œ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëŠ˜ë¦´ ìˆ˜ ìˆê² ìœ¼ë‚˜, ë°ì´í„°ë¥¼ ëŠ˜ë¦¬ë”ë¼ë„, ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë‹¤ì‹œ í•™ìŠµí•˜ëŠ”ë°ì—ëŠ” ì‹œê°„ì´ ì—†ìœ¼ë¯€ë¡œ, ì—­ë²ˆì—­ ì¦ê°•ì€ ì´ë²ˆëŒ€íšŒì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë¡œ í–ˆë‹¤.

ê¸°ì¡´ì˜ ë°©ì‹ì€ Traindataë¥¼ valid,trainìœ¼ë¡œ ë‹¤ì‹œ ë‚˜ëˆ ì„œ í•™ìŠµìš©ê³¼ ê²€ì¦ë°ì´í„°ë¥¼ ë‚˜ëˆ ì„œ í•™ìŠµì„ ì§„í–‰í–ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ìµœì¢… ëª¨ë¸ì„ í•™ìŠµí• ë–„ëŠ” ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ì„œ ë‹¤ì‹œ í•™ìŠµí•œ í›„ ì•™ìƒë¸”ì„ í•œë‹¤ê³  í•œë‹¤. ì´ë˜í•œ ì•™ìƒë¸”ì„ í•˜ë ¤ë©´ ìµœì†Œ 3ê°œì˜ ëª¨ë¸ì„ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ì„œ ë‹¤ì‹œ í•™ìŠµì„ í•´ì•¼í•˜ëŠ”ë°, ì½”ë“œê¹Œì§€ëŠ” êµ¬í˜„í–ˆìœ¼ë‚˜, ì‹œê°„ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ë¡œ ë°˜ì˜í•˜ì§€ëŠ” ëª»í–ˆë‹¤.

# Weighted Model Ensemble

```python
# === Ensemble Inference Block (Weighted Averaging) ===
# Replace the original inference cell (ID: 079c8f40) with this code.

import torch
import numpy as np
import os
from tqdm.auto import tqdm # Import tqdm for progress bar
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
import shutil # Import shutil for directory removal

print(f"í˜„ì¬ ë””ë°”ì´ìŠ¤: {device}")

# --- Load Test Data ---
df_test = pd.read_csv("data/test.csv")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_test)} ìƒ˜í”Œ")

# --- Apply Preprocessing ---
# Use the 'preprocessor' instance fitted during training
print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© ì¤‘...")
test_texts = df_test["review"].tolist()
# Use transform only, assumes preprocessor is already fitted
# Ensure the preprocessor was fitted during the training phase
if not preprocessor.is_fitted:
    # If the preprocessor wasn't fitted (e.g., running inference separately),
    # you might need to fit it here or load a saved preprocessor state.
    # For now, we'll just apply basic preprocessing.
    print("Warning: Preprocessorê°€ fitë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì „ì²˜ë¦¬ë§Œ ì ìš©í•©ë‹ˆë‹¤.")
    test_processed = preprocessor.basic_preprocess(test_texts)
else:
    test_processed = preprocessor.transform(test_texts) # Apply transform including rare word removal if fitted
print("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")

# --- List of Trained Model Paths ---
# Ensure these paths match where models were saved in the training block
# ìˆ˜ì •: best_model ëŒ€ì‹  swa_model ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
model_names = [
    "./tapted_klue_roberta_base",
    "./tapted_klue_bert_base",
    "./tapted_kykim_bert_kor_base",
    "./tapted_beomi_kcbert_base",
    "./tapted_koelectra_base_v3"
]
model_paths = [
    f"./results_{model_name.split('/')[-1].replace('-', '_')}/swa_model_{model_name.split('/')[-1].replace('-', '_')}" # <-- swa_model ë¡œ ë³€ê²½
    for model_name in model_names # Use the same model_names list from training
]
print(model_paths)

# model_paths = [
#     f"./results_{model_name.split('/')[-1].replace('-', '_')}/best_model_{model_name.split('/')[-1].replace('-', '_')}" # <-- swa_model ë¡œ ë³€ê²½
#     for model_name in model_names # Use the same model_names list from training
# ]

# --- Define Model Weights (Based on Validation Performance) ---
# ê° ëª¨ë¸ì˜ ê²€ì¦ ë°ì´í„°ì…‹ F1 score (ë˜ëŠ” Accuracy) ê°’ìœ¼ë¡œ ë°˜ë“œì‹œ êµì²´í•´ì•¼ í•©ë‹ˆë‹¤.
# ì´ê±° f1ìœ¼ë¡œ í• ì§€ accuracyë¡œí• ì§€ ë‘˜ë‹¤ í•´ë³´ê¸°
# model_paths ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë¸ ìˆœì„œì™€ ë™ì¼í•˜ê²Œ ê°’ì„ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤.
model_performance_scores = {
    model_paths[0]: 0.8114, #roberta
    model_paths[1]: 0.8056, # ì˜ˆì‹œ: tapted_klue_bert_base SWA ëª¨ë¸ì˜ ê²€ì¦ F1
    model_paths[2]: 0.8179, # ì˜ˆì‹œ: tapted_kykim_bert_kor_base SWA ëª¨ë¸ì˜ ê²€ì¦ F1
    model_paths[3]: 0.7961,  # ì˜ˆì‹œ: tapted_beomi_kcbert_base SWA ëª¨ë¸ì˜ ê²€ì¦ F1
    model_paths[4]: 0.8077 #koelectra
    # ë‹¤ë¥¸ ëª¨ë¸ ì¶”ê°€ ì‹œ ì—¬ê¸°ì— ì„±ëŠ¥ ì ìˆ˜ ì¶”ê°€
}

# --- Ensemble Inference ---
all_logits = []
successful_model_paths = [] # ì¶”ë¡ ì— ì„±ê³µí•œ ëª¨ë¸ ê²½ë¡œ ì €ì¥
print("\n" + "=" * 50)
print("ì•™ìƒë¸” ì¶”ë¡  ì‹œì‘...")
print("=" * 50)

for model_path in tqdm(model_paths, desc="ëª¨ë¸ë³„ ì¶”ë¡  ì§„í–‰"):
    if not os.path.exists(model_path):
        print(f"Warning: ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}. ì´ ëª¨ë¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    print(f"\nëª¨ë¸ ë¡œë”©: {model_path}")
    try:
        # Load tokenizer and model for the current path
        current_tokenizer = AutoTokenizer.from_pretrained(model_path)
        current_model = AutoModelForSequenceClassification.from_pretrained(model_path)
        current_model.to(device)
        current_model.eval()

        # Create test dataset with the current tokenizer
        test_dataset_current = ReviewDataset(
            pd.Series(test_processed), # Pass processed text as a Series
            None, # No labels for test set
            current_tokenizer,
            CHOSEN_MAX_LENGTH
        )

        # Define minimal TrainingArguments for prediction
        temp_output_dir = f"./temp_prediction_{os.path.basename(model_path)}"
        predict_args = TrainingArguments(
            output_dir=temp_output_dir,
            per_device_eval_batch_size=BATCH_SIZE_EVAL * 2, # Increase batch size for faster inference
            dataloader_num_workers=2,
            remove_unused_columns=False, # Keep columns needed by dataset
            fp16=torch.cuda.is_available(),
            logging_steps=len(test_dataset_current) // (BATCH_SIZE_EVAL * 2) + 1 # Reduce logging
        )

        # Initialize Trainer for prediction
        pred_trainer = Trainer(
            model=current_model,
            args=predict_args,
            # tokenizer=current_tokenizer, # Trainerì—ì„œëŠ” tokenizer ì—†ì–´ë„ ë¨
            data_collator=DataCollatorWithPadding(tokenizer=current_tokenizer),
        )

        # Get predictions (logits)
        print(f"ëª¨ë¸ ì¶”ë¡  ì¤‘: {model_path}")
        predictions = pred_trainer.predict(test_dataset_current)
        logits = predictions.predictions # Logits are in predictions.predictions
        all_logits.append(logits)
        successful_model_paths.append(model_path) # ì„±ê³µí•œ ëª¨ë¸ ê²½ë¡œ ì¶”ê°€
        print(f"ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ: {model_path}")

        # Clean up memory
        del current_model
        del current_tokenizer
        del pred_trainer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        # Clean up temporary directory
        if os.path.exists(temp_output_dir):
             shutil.rmtree(temp_output_dir)

    except Exception as e:
        print(f"Error during inference for model {model_path}: {e}")
        # Decide whether to continue or stop if one model fails
        # continue # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ëª¨ë¸ ê±´ë„ˆë›°ê¸°

# --- Weighted Averaging Logits ---
if not all_logits:
    raise ValueError("ì¶”ë¡  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” ì¶”ë¡  ê³¼ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

print("\në¡œì§“(logits) ê°€ì¤‘ í‰ê·  ê³„ì‚° ì¤‘...")
# Convert list of numpy arrays to a numpy array for easier calculation
all_logits_np = np.array(all_logits)

# Filter performance scores for successfully loaded models
relevant_scores = [model_performance_scores[path] for path in successful_model_paths if path in model_performance_scores]

if len(relevant_scores) != all_logits_np.shape[0]:
    print(f"Warning: ì¶”ë¡ ëœ ëª¨ë¸ ìˆ˜({all_logits_np.shape[0]})ì™€ ìœ íš¨í•œ ì„±ëŠ¥ ì ìˆ˜ ê°œìˆ˜({len(relevant_scores)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ë‹¨ìˆœ í‰ê· ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    # Fallback to simple mean if weights don't match (e.g., model failed or score missing)
    average_logits = np.mean(all_logits_np, axis=0)
else:
    # Calculate weights based on performance scores (Normalization)
    total_score = sum(relevant_scores)
    if total_score == 0: # Avoid division by zero if all scores are 0
        print("Warning: ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ì ìˆ˜ê°€ 0ì…ë‹ˆë‹¤. ë™ì¼ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        weights = np.ones(len(relevant_scores)) / len(relevant_scores)
    else:
        weights = np.array([score / total_score for score in relevant_scores])
    print(f"ì‚¬ìš©ëœ ëª¨ë¸ ê²½ë¡œ: {successful_model_paths}")
    print(f"í•´ë‹¹ ëª¨ë¸ ê°€ì¤‘ì¹˜: {weights}")

    # Calculate the weighted average logits across models (axis=0)
    average_logits = np.average(all_logits_np, axis=0, weights=weights)

print(f"ê°€ì¤‘ í‰ê·  ë¡œì§“ ê³„ì‚° ì™„ë£Œ. í˜•íƒœ: {average_logits.shape}")

# --- Final Prediction ---
predicted_labels = np.argmax(average_logits, axis=1)
print(f"ìµœì¢… ì˜ˆì¸¡ ë ˆì´ë¸” ìƒì„± ì™„ë£Œ: {len(predicted_labels)}ê°œ")

# --- Add predictions to the test DataFrame ---
df_test["pred"] = predicted_labels
print(f"\ndf_testì— pred ì»¬ëŸ¼ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. í˜•íƒœ: {df_test.shape}")

# --- Analyze Prediction Distribution ---
unique_predictions, counts = np.unique(predicted_labels, return_counts=True)
print("\nìµœì¢… ì˜ˆì¸¡ ë¶„í¬:")
# Ensure LABEL_MAPPING is defined in your environment
if 'LABEL_MAPPING' not in globals():
    LABEL_MAPPING = {0: "ê°•í•œ ë¶€ì •", 1: "ì•½í•œ ë¶€ì •", 2: "ì•½í•œ ê¸ì •", 3: "ê°•í•œ ê¸ì •"} # Define if not present

for pred, count in zip(unique_predictions, counts):
    percentage = (count / len(predicted_labels)) * 100
    class_name = LABEL_MAPPING.get(pred, f"í´ë˜ìŠ¤ {pred}")
    print(f"   {class_name} ({pred}): {count:,}ê°œ ({percentage:.1f}%)")

print("\n" + "=" * 50)
print("ì•™ìƒë¸” ì¶”ë¡  ì™„ë£Œ!")
print("=" * 50)

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# Note: The code for creating and saving the submission file remains the same
# It should follow this block, using the df_test DataFrame which now has the 'pred' column
```

![image](/assets/images/2025-11-02-15-21-27.png)

![image](/assets/images/2025-11-02-15-21-37.png)

ìµœì¢… ëª©í‘œê°€ test/accë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì¸ë§Œí¼ ê° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” ê° ë‹¨ì¼ëª¨ë¸ì˜ test/accë¥¼ í™•ì¸í•˜ê³ , ì´ accë¥¼ ì •ê·œí™” í•œ í›„ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í–ˆë‹¤.

![image](/assets/images/2025-11-02-15-21-43.png)

ë‹¨ì¼ëª¨ë¸ë¡œì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì˜ë‚˜ì™”ë˜ RoBERTaì™€ Kykim-BERTë‘ê°œë¥¼ ì•™ìƒë¸”í•œê²ƒë³´ë‹¤, ì „ì²´ ëª¨ë¸ 5ê°œë¥¼ ì•™ìƒë¸”í•œ ê²°ê³¼ê°€ ë” ì„±ëŠ¥ì´ ì˜ë‚˜ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. Baggingì— ì˜í•´ ì„œë¡œë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ì¼ë°˜ì ìœ¼ë¡œ ì„ í˜•ì ìœ¼ë¡œ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.

# ì „ì²´ ë°ì´í„°ë¥¼ ì´ìš©í•œ í•™ìŠµ

```python
# === ìµœì¢… ëª¨ë¸ ì¬í•™ìŠµ (ì „ì²´ ë°ì´í„° í™œìš©) ë¸”ë¡ ===
# ì´ ë¸”ë¡ì€ ì•™ìƒë¸” í›ˆë ¨(ID: 176174ee) ì´í›„ì— ì‹¤í–‰í•©ë‹ˆë‹¤.

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, DataCollatorWithPadding, Trainer
import os
import shutil # ì¶”ê°€: ë””ë ‰í† ë¦¬ ê´€ë¦¬ìš©

print("\n" + "="*60)
print("ğŸš€ ì „ì²´ ë°ì´í„°(í•™ìŠµ+ê²€ì¦)ë¥¼ ì‚¬ìš©í•œ ìµœì¢… ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘")
print("="*60 + "\n")

# --- ì„¤ì •ê°’ ---

# 1. ì¬í•™ìŠµí•  ê¸°ë°˜ ëª¨ë¸ ì´ë¦„ ë° ê²½ë¡œ ì„ íƒ
#    !!! ì¤‘ìš”: ì´ˆê¸° í›ˆë ¨/ê²€ì¦ì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ ëª¨ë¸ì˜ ì´ë¦„ì„ ì§€ì •í•˜ì„¸ìš”.
#    (ì˜ˆ: Wandb ë¡œê·¸ì—ì„œ eval_lossê°€ ê°€ì¥ ë‚®ê±°ë‚˜ eval_f1/eval_accuracyê°€ ê°€ì¥ ë†’ì•˜ë˜ ëª¨ë¸)
#    SWA ëª¨ë¸ì´ ë” ì¢‹ì•˜ë‹¤ë©´ í•´ë‹¹ ëª¨ë¸ ê²½ë¡œë¥¼ initial_model_pathë¡œ ì§ì ‘ ì§€ì •í•´ë„ ë©ë‹ˆë‹¤.
BEST_PERFORMING_MODEL_NAME = "./tapted_kykim_bert_kor_base" # <--- ì‹¤ì œ ìµœì  ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš” (ì˜ˆì‹œ)
best_model_short_name = BEST_PERFORMING_MODEL_NAME.split('/')[-1].replace('-', '_')

# ì´ˆê¸° í›ˆë ¨ ê²°ê³¼ì—ì„œ 'best_model' ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
# SWA ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì¬í•™ìŠµí•˜ë ¤ë©´ ì´ ê²½ë¡œë¥¼ swa_model ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”.
# initial_model_path = f"./results_{best_model_short_name}/best_model_{best_model_short_name}"
# ì˜ˆ: SWA ê¸°ë°˜ ì¬í•™ìŠµ ì‹œ
initial_model_path = f"./results_{best_model_short_name}/swa_model_{best_model_short_name}"

print(f"ì„ íƒëœ ê¸°ë°˜ ëª¨ë¸: {BEST_PERFORMING_MODEL_NAME}")
print(f"ê¸°ë°˜ ëª¨ë¸ ê²½ë¡œ: {initial_model_path}")

if not os.path.exists(initial_model_path):
    raise FileNotFoundError(f"ê¸°ë°˜ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {initial_model_path}. ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì´ì „ í›ˆë ¨ì„ ì™„ë£Œí•˜ì„¸ìš”.")

# 2. ì „ì²´ ë°ì´í„° ì¬í•™ìŠµ ì‹œ ì‚¬ìš©í•  ì—í¬í¬ ìˆ˜
#    !!! ì¤‘ìš”: ì´ˆê¸° í›ˆë ¨ ì¤‘ ê²€ì¦ ë°ì´í„°ì—ì„œ ìµœì  ì„±ëŠ¥(ì˜ˆ: ê°€ì¥ ë‚®ì€ eval_loss)ì„ ë³´ì˜€ë˜
#    ì—í¬í¬ ìˆ˜ë¥¼ í™•ì¸í•˜ì—¬ ì„¤ì •í•˜ì„¸ìš”. (Wandb ë“± ë¡œê·¸ í™•ì¸ í•„ìš”)
FINAL_TRAIN_EPOCHS = 5 # <--- ì‹¤ì œ ìµœì  ì—í¬í¬ ìˆ˜ë¡œ ë³€ê²½í•˜ì„¸ìš” (ì˜ˆì‹œ)
print(f"ì¬í•™ìŠµ ì—í¬í¬ ìˆ˜: {FINAL_TRAIN_EPOCHS}")

# 3. ìµœì¢… ëª¨ë¸ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
final_output_dir = f"./final_model_{best_model_short_name}_fulldata"
print(f"ìµœì¢… ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {final_output_dir}\n")

# 4. ì‚¬ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì´ˆê¸° í›ˆë ¨ì˜ ìµœì ê°’ ìœ ì§€)
#    ì´ ê°’ë“¤ì€ ì´ì „ ì…€(ID: b5ba58df) ë° í›ˆë ¨ ë£¨í”„(ID: 176174ee)ì—ì„œ ì‚¬ìš©ëœ ê°’ê³¼ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
FINAL_LEARNING_RATE = LEARNING_RATE
FINAL_WEIGHT_DECAY = WEIGHT_DECAY
FINAL_WARMUP_STEPS = WARMUP_STEPS
FINAL_BATCH_SIZE_TRAIN = BATCH_SIZE_TRAIN
FINAL_GRAD_ACCUM_STEPS = 4 # ì´ˆê¸° í›ˆë ¨ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì • (training_argsì—ì„œ í™•ì¸)
FINAL_LABEL_SMOOTHING = 0.03442149572139974 # ì´ˆê¸° í›ˆë ¨ ê°’ (training_argsì—ì„œ í™•ì¸)
FINAL_LR_SCHEDULER = "constant" # ì´ˆê¸° í›ˆë ¨ ê°’ (training_argsì—ì„œ í™•ì¸)
# Focal Loss íŒŒë¼ë¯¸í„° (CustomTrainerWithFocalLoss ì´ˆê¸°í™” ì‹œ ì‚¬ìš©ëœ ê°’)
FINAL_FOCAL_ALPHA = [1,1,1,1]
FINAL_FOCAL_GAMMA = 4.194768435552584

# --- 1. í•™ìŠµ + ê²€ì¦ ë°ì´í„° í•©ì¹˜ê¸° ---
print("ğŸ“Š í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° í•©ì¹˜ëŠ” ì¤‘...")
# train_dataì™€ val_dataëŠ” ì´ì „ ë°ì´í„° ë¶„í•  ì…€(ID: e6e4ca17)ì—ì„œ ìƒì„±ëœ ë³€ìˆ˜ì…ë‹ˆë‹¤.
if 'train_data' not in globals() or 'val_data' not in globals():
    raise NameError("train_data ë˜ëŠ” val_data ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ë°ì´í„° ë¶„í•  ì…€ì„ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

full_train_data = pd.concat([train_data, val_data], ignore_index=True)
print(f"   ì „ì²´ í•™ìŠµ ë°ì´í„° í¬ê¸°: {len(full_train_data):,}ê°œ")
print(f"   í•©ì³ì§„ ë°ì´í„° ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ 3ê°œ):\n{full_train_data.head(3)}\n")

# --- 2. ê¸°ë°˜ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---
print(f"ğŸ”© ê¸°ë°˜ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©: {initial_model_path}")
try:
    final_tokenizer = AutoTokenizer.from_pretrained(initial_model_path)
    final_model = AutoModelForSequenceClassification.from_pretrained(
        initial_model_path,
        num_labels=NUM_CLASSES, # í´ë˜ìŠ¤ ìˆ˜ëŠ” ë™ì¼
    )
    final_model.to(device) # GPUë¡œ ì´ë™
    print("   ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

    # íŠ¹ìˆ˜ í† í° ì¶”ê°€ ë° ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ (ì´ˆê¸° í›ˆë ¨ê³¼ ë™ì¼í•˜ê²Œ ìˆ˜í–‰)
    num_added = final_tokenizer.add_tokens(NEW_SPECIAL_TOKENS)
    if num_added > 0:
        final_model.resize_token_embeddings(len(final_tokenizer))
        print(f"   {num_added}ê°œì˜ íŠ¹ìˆ˜ í† í° ì¶”ê°€ ë° ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ ì™„ë£Œ.")

except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì¬í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    # í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì„œ ì¤‘ë‹¨
    raise e

# --- 3. ì „ì²´ ë°ì´í„°ì…‹ ìƒì„± ---
print("\nğŸ“š ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ PyTorch ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
# ReviewDataset í´ë˜ìŠ¤ëŠ” ì´ì „ ì…€(ID: 1751489f)ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
full_train_dataset = ReviewDataset(
    full_train_data["review"],  # í•©ì³ì§„ ë°ì´í„°ì˜ ì „ì²˜ë¦¬ëœ ë¦¬ë·° ì‚¬ìš©
    full_train_data["label"],   # í•©ì³ì§„ ë°ì´í„°ì˜ ë ˆì´ë¸” ì‚¬ìš©
    final_tokenizer,
    CHOSEN_MAX_LENGTH,
)
print(f"   ì „ì²´ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(full_train_dataset):,}ê°œ")

# --- 4. í›ˆë ¨ ì„¤ì • (TrainingArguments) ë³€ê²½ ---
#    - ê²€ì¦(evaluation) ë¹„í™œì„±í™”
#    - ìµœì  ëª¨ë¸ ë¡œë”© ë¹„í™œì„±í™”
#    - Wandb ë¡œê¹… ë¹„í™œì„±í™” (ì„ íƒ ì‚¬í•­)
print("\nâš™ï¸  ìµœì¢… í›ˆë ¨ì„ ìœ„í•œ TrainingArguments ì„¤ì • ì¤‘...")
final_training_args = TrainingArguments(
    output_dir=final_output_dir,          # ìƒˆë¡œìš´ ì¶œë ¥ ë””ë ‰í† ë¦¬
    num_train_epochs=FINAL_TRAIN_EPOCHS,  # ê²°ì •ëœ ìµœì¢… ì—í¬í¬ ìˆ˜
    per_device_train_batch_size=FINAL_BATCH_SIZE_TRAIN, # ê¸°ì¡´ í›ˆë ¨ ë°°ì¹˜ í¬ê¸°
    gradient_accumulation_steps=FINAL_GRAD_ACCUM_STEPS, # ê¸°ì¡´ ê°’
    # --- ê²€ì¦ ê´€ë ¨ ì„¤ì • ì œê±°/ë³€ê²½ ---
    eval_strategy="no",                   # ê²€ì¦ ì•ˆ í•¨
    # --- ì €ì¥ ê´€ë ¨ ì„¤ì • ---
    save_strategy="epoch",                # ì—í¬í¬ë§ˆë‹¤ ì €ì¥ (ë§ˆì§€ë§‰ ëª¨ë¸ë§Œ í•„ìš”í•˜ë©´ "no" ë˜ëŠ” ë§ˆì§€ë§‰ ì—í¬í¬ë§Œ ì €ì¥)
    save_total_limit=1,                   # ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ë§Œ ì €ì¥ (í•„ìš”ì‹œ ëŠ˜ë¦¼)
    load_best_model_at_end=False,         # ìµœì  ëª¨ë¸ ë¡œë”© ì•ˆ í•¨ (ê²€ì¦ ì—†ìœ¼ë¯€ë¡œ)
    metric_for_best_model= None,  
    # --- ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìœ ì§€ ---
    warmup_steps=FINAL_WARMUP_STEPS,
    weight_decay=FINAL_WEIGHT_DECAY,
    learning_rate=FINAL_LEARNING_RATE,
    label_smoothing_factor=FINAL_LABEL_SMOOTHING,
    lr_scheduler_type=FINAL_LR_SCHEDULER,
    # --- ê¸°íƒ€ ì„¤ì • ---
    logging_steps=100,                    # ë¡œê·¸ ê°„ê²© (ë°ì´í„° ì–‘ ê³ ë ¤í•˜ì—¬ ì¡°ì • ê°€ëŠ¥)
    seed=RANDOM_STATE,                    # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
    fp16=torch.cuda.is_available(),       # FP16 ì‚¬ìš© (GPU ì‚¬ìš© ì‹œ)
    dataloader_num_workers=2,             # ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜
    remove_unused_columns=False,          # ReviewDatasetì—ì„œ ì‚¬ìš©í•˜ëŠ” ì»¬ëŸ¼ ìœ ì§€
    save_safetensors=True,                # Safetensors í˜•ì‹ìœ¼ë¡œ ì €ì¥ ê¶Œì¥
    logging_first_step=True,
    report_to="none",                     # ìµœì¢… ì¬í•™ìŠµì€ Wandb ë¡œê¹… ì•ˆ í•¨ (ì„ íƒ ì‚¬í•­)
    # push_to_hub=False, # í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œ í‘¸ì‹œ ì•ˆ í•¨
)
print("   TrainingArguments ì„¤ì • ì™„ë£Œ.")

# --- 5. ìµœì¢… Trainer ì´ˆê¸°í™” ---
#    - Focal Loss ì‚¬ìš© (CustomTrainerWithFocalLoss í´ë˜ìŠ¤ í•„ìš”)
#    - ê²€ì¦ ë°ì´í„°ì…‹, ë©”íŠ¸ë¦­ ê³„ì‚°, ì½œë°± ì œê±°
print("\nğŸ”§ ìµœì¢… Trainer (CustomTrainerWithFocalLoss) ì´ˆê¸°í™” ì¤‘...")
# CustomTrainerWithFocalLoss í´ë˜ìŠ¤ëŠ” ì´ì „ ì…€(ID: eca0c60f)ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
final_trainer = CustomTrainerWithFocalLoss(
    model=final_model,                  # ë¡œë“œëœ ê¸°ë°˜ ëª¨ë¸
    args=final_training_args,           # ìµœì¢… í›ˆë ¨ ì„¤ì •
    train_dataset=full_train_dataset,   # ì „ì²´ í•™ìŠµ ë°ì´í„°ì…‹
    eval_dataset=None,                  # ê²€ì¦ ë°ì´í„°ì…‹ ì—†ìŒ
    tokenizer=final_tokenizer,          # DataCollatorì— í•„ìš”
    num_classes=NUM_CLASSES,            # í´ë˜ìŠ¤ ìˆ˜
    focal_loss_alpha=FINAL_FOCAL_ALPHA, # ê¸°ì¡´ Focal Loss alpha
    focal_loss_gamma=FINAL_FOCAL_GAMMA, # ê¸°ì¡´ Focal Loss gamma
    data_collator=DataCollatorWithPadding(tokenizer=final_tokenizer), # íŒ¨ë”© ì²˜ë¦¬
    compute_metrics=None,               # ê²€ì¦ ì•ˆ í•˜ë¯€ë¡œ ë©”íŠ¸ë¦­ ê³„ì‚° ë¶ˆí•„ìš”
    callbacks=None,                     # EarlyStopping ë“± ì½œë°± ì œê±°
)
print("   Trainer ì´ˆê¸°í™” ì™„ë£Œ.")

# --- 6. ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ ì‹¤í–‰ ---
print(f"\nğŸ’ª ì „ì²´ ë°ì´í„°ë¡œ {FINAL_TRAIN_EPOCHS} ì—í¬í¬ ì¬í•™ìŠµ ì‹œì‘...")
try:
    train_result = final_trainer.train()
    print("   ì¬í•™ìŠµ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ.")
    print(f"   ì´ í›ˆë ¨ ì‹œê°„: {train_result.metrics['train_runtime']:.2f}ì´ˆ")
    print(f"   ìµœì¢… í›ˆë ¨ ì†ì‹¤: {train_result.metrics['train_loss']:.4f}")
except KeyboardInterrupt:
    print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¬í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"\nâŒ ì¬í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    # í•„ìš”í•œ ê²½ìš° ì˜¤ë¥˜ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
    raise e

# --- 7. ìµœì¢… ëª¨ë¸ ì €ì¥ ---
print(f"\nğŸ’¾ í›ˆë ¨ ì™„ë£Œëœ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘: {final_output_dir}")
try:
    # ê¸°ì¡´ ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ì‚­ì œ (ë®ì–´ì“°ê¸° ìœ„í•´)
    if os.path.exists(final_output_dir):
        print(f"   ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ: {final_output_dir}")
        shutil.rmtree(final_output_dir)

    final_trainer.save_model(final_output_dir) # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ì„¤ì • ì €ì¥
    final_tokenizer.save_pretrained(final_output_dir) # í† í¬ë‚˜ì´ì € íŒŒì¼ ì €ì¥
    print(f"   ìµœì¢… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ: {final_output_dir}")

    # ì €ì¥ëœ íŒŒì¼ í™•ì¸
    if os.path.exists(final_output_dir):
        saved_files = os.listdir(final_output_dir)
        print(f"   ì €ì¥ëœ íŒŒì¼ ëª©ë¡: {saved_files}")

except Exception as e:
    print(f"âŒ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    # í•„ìš”í•œ ê²½ìš° ì˜¤ë¥˜ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€

# --- 8. ë©”ëª¨ë¦¬ ì •ë¦¬ ---
print("\nğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
del final_model
del final_tokenizer
del final_trainer
del full_train_dataset
del full_train_data
if 'train_result' in globals(): del train_result # í›ˆë ¨ ê²°ê³¼ ê°ì²´ë„ ì‚­ì œ
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("   GPU ìºì‹œ ë¹„ì›€ ì™„ë£Œ.")
print("   ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ.")

print("\n" + "="*60)
print("âœ… ì „ì²´ ë°ì´í„° ì¬í•™ìŠµ ë° ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
print(f"   ìµœì¢… ëª¨ë¸ì€ '{final_output_dir}' ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("   ì¶”ë¡ (Inference) ì‹œ ì´ ê²½ë¡œì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
print("="*60)
```

ê° ëª¨ë¸ì„ ì´ì „ì— í•™ìŠµí•˜ëŠ” trainì½”ë“œì™€ ê°™ì€ ë°©ì‹ì„ ê³µìœ í•˜ë˜, train, validation ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì§€ ì•Šê³  ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•˜ëŠ” ì½”ë“œë¥¼ êµ¬í˜„í–ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ, ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì„ ê³¨ë¼ì„œ, ë‹¤ì‹œ ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµì„ í•´ì„œ ì œì¶œí•œë‹¤ê³  í•œë‹¤. í•˜ì§€ë§Œ, ì‹œê°„ì´ ë¶€ì¡±í•´ì„œ ë‹¤ì‹œ ëª¨ë¸ë“¤ì„ ì¬í•™ìŠµí•´ë³´ì§€ëŠ” ëª»í–ˆë‹¤.

# ê²°ê³¼



![image](/assets/images/2025-11-02-15-22-09.png)
![image](/assets/images/2025-11-02-15-22-23.png)
![image](/assets/images/2025-11-02-15-22-29.png)

TAPT, FocalLossì˜ $\alpha$ê°€ì¤‘ì¹˜, SWAí•  ëª¨ë¸ì˜ ìˆ˜ë“±ì„ ì´ì „ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„¸íŒ…ê³¼ ë‹¤ë¥´ê²Œ í•´ì„œì¸ì§€, ì´ì „ì— BERTê¸°ë°˜ 3ê°œëª¨ë¸ì„ ì•™ìƒë¸”í–ˆì„ë•Œë³´ë‹¤ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ê±°ë‚˜ ì†Œí­í•˜ë½í–ˆì—ˆë‹¤. ë”°ë¼ì„œ ì´ì „ì— ì‹¤í—˜í–ˆë˜ ëª¨ë¸ë“¤ì„ ìµœì¢… ì œì¶œí–ˆê³ , private scoreëŠ” 0.8301ë¡œ Final LeaderboardëŠ” 98ìœ„ë¡œ ë§ˆë¬´ë¦¬í–ˆë‹¤. ì²« ê²½ì§„ëŒ€íšŒ ì°¸ì—¬ë¼ì„œ ì¢‹ì€ ì„±ê³¼ëŠ” ë‚´ì§€ ëª»í–ˆì§€ë§Œ ì²˜ìŒì•Œê³ , ì–»ì–´ê°€ëŠ”ê²ƒë“¤ì´ ë§ì€ ëŒ€íšŒê²½í—˜ì´ì—ˆë‹¤.

í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•˜ëŠ” ë„ì¤‘ì— TAPTê¸°ë²•ìœ¼ë¡œ ëª¨ë¸ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì•Œê²Œë˜ì—ˆê³ , BackTrainslationì„ ì´ìš©í•´ì„œ íš¨ê³¼ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¦ê°•í•˜ê±°ë‚˜, ì „ì²´ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ì„œ í•™ìŠµí•´ì„œ ì„±ëŠ¥ì„ í–¥ìƒí•´ë³´ëŠ” ì‹œë„ë“¤ì„ ì¶”ê°€ë¡œ í•´ë³´ê³ ì‹¶ì—ˆì§€ë§Œ, ì‹œê°„ì´ ë¶€ì¡±í–ˆì—ˆë˜ê²ƒ ê°™ë‹¤. ëª¨ë“  ì‹¤í—˜ê³¼ ì‹œë„ë“¤ì„ í•´ë³¼ ìˆ˜ëŠ” ì—†ë‹¤ëŠ”ê²ƒì„ ê¹¨ë‹¬ì•˜ë‹¤. ê°€ì¥ ì¤‘ìš”í•œê²ƒì„ ìœ„ì£¼ë¡œ ì‹¤í—˜ ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•´ì•¼í•˜ê³ , ì¤‘ìš”í•œê²ƒë¶€í„° ì‹¤í—˜ ìë™í™” ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ë§Œë“¤ì–´ì„œ 24ì‹œê°„ GPUë¥¼ êµ´ë¦¬ëŠ”ê²Œ ì¤‘ìš”í•˜ë‹¤ëŠ”ê²ƒì„ ê¹¨ë‹¬ì•˜ë‹¤.

- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì€ ê°€ì¥ ë§ˆì§€ë§‰ê³¼ì •ì— ì‹œì‘í•´ì„œ ëŒ€íšŒê°€ ëë‚˜ê¸° ì§ì „ê¹Œì§€ í•´ì•¼í•œë‹¤.

í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ë¯¸ë¦¬í•˜ë©´, TAPTì ìš©ì´ë‚˜, ë°ì´í„° ì¦ê°•ë“±ì„ ì ìš©í–ˆì„ë•Œ, ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ë‹¤ì‹œ ë‹¬ë¼ì§€ê²Œ ë˜ë¯€ë¡œ ë˜ í•´ì•¼í•˜ëŠ” ë¶ˆìƒì‚¬ê°€ ìƒê¸´ë‹¤. EDA, FeatureEngineeringë“± ëª¨ë¸ë§ì„ ì‚¬ì „ì— ë¨¼ì € ì¶©ë¶„íˆ ì§„í–‰í•œ í›„ ëŒ€íšŒê°€ ëë‚˜ê¸° ì§ì „ê¹Œì§€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•´ì•¼í•œë‹¤ëŠ”ê²ƒì„ ì•Œê²Œë˜ì—ˆë‹¤.

- ìš°ì„  ì œì¼ë¨¼ì € ê°€ëŠ¥í•œ SOTAëª¨ë¸ë“¤ì„ ì‹¤í—˜í•´ë³´ê³  ì‚¬ìš©í•  ëª¨ë¸ì„ ê³ ë¥´ì. ê·¸ë¦¬ê³  ì‹¤í—˜ ìš°ì„ ìˆœìœ„ë¥¼ ë¨¼ì € ì •í•˜ì.

ì–´ë–¤ëª¨ë¸ì„ ì‚¬ìš©í• ê²ƒì¸ê°€ë¥¼ ê°€ì¥ ë¨¼ì € ê²°ì •í•´ì•¼ í•œë‹¤. ì´ë²ˆ ëŒ€íšŒëŠ” 5ê°œ ëª¨ë¸ì„ í•œì •ì ìœ¼ë¡œ ì‚¬ìš©í—€ê¸° ë–„ë¬¸ì—, ê°„ë‹¨í–ˆì§€ë§Œ, ë‹¤ë¥¸ëŒ€íšŒì—ì„œëŠ” ë§ˆì§€ë§‰ì— ì•™ìƒë¸”í•  ëª¨ë¸ë“¤ì„ ë¨¼ì € ì„ íƒí•˜ê³ , í•´ë‹¹ ëª¨ë¸ë“¤ì— ë§ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì§„í–‰í•´ì•¼í•œë‹¤. ê·¸ë¦¬ê³  ê¸°ê°„ë‚´ì— ëª¨ë“  ì‹¤í—˜ë“¤ì„ í•´ë³¼ ìˆ˜ëŠ” ì—†ë‹¤. ê°€ì¥ ì¤‘ìš”í•œ ì‹¤í—˜ë¶€í„° ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•˜ê³ , ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì´ìš©í•´ì„œ ì‹¤í—˜ê´€ë¦¬ë¥¼ ìë™í™”í•´ì•¼í•œë‹¤. í•˜ë£¨ì— 10ë²ˆì˜ ì œì¶œì„ í•  ìˆ˜ ìˆì—ˆì§€ë§Œ, ë§ì€ ì‹œë„ë“¤ì„ í•´ë³´ì§€ ëª»í•œê²ƒì— ëŒ€í•œ ì•„ì‰¬ì›€ì´ ë‚¨ëŠ”ë‹¤.