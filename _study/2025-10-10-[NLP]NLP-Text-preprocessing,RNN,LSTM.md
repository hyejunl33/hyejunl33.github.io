---
title: "[NLP][ë¯¸ì…˜]NLP:í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, RNN,LSTM êµ¬í˜„"
date: 2025-10-10
tags:
  -[NLP]NLP:í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, RNN,LSTM êµ¬í˜„
  - ìœ„í´ë¦¬ ë¯¸ì…˜
  - í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
  - Xavierì´ˆê¸°í™”
  - RNN, LSTM êµ¬í˜„
excerpt: "[NLP][ë¯¸ì…˜]NLP:í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, RNN,LSTM êµ¬í˜„"
math: true
---

# [NLP][ë¯¸ì…˜]NLP:í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, RNN,LSTM êµ¬í˜„

- Text PreProcessing, Embedding êµ¬í˜„
- RNN cellì„ êµ¬í˜„í•˜ê³ , RNN ëª¨ë¸ì„ êµ¬í˜„
- LSTM cellì„ êµ¬í˜„í•˜ê³ , LSTM ëª¨ë¸êµ¬í˜„

# Text Preprocessing and Tokenization

- ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ wordë‹¨ìœ„ë¡œ êµ¬ë¶„í•´ì„œ tokenìœ¼ë¡œ preprocessingì„ í•´ë³´ì

```python
def simple_tokenize(text):
    #1. í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê¸°
    text = text.lower()

    #2. ..!?;,ëŠ” í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì•ë’¤ë¡œ ê³µë°±ì¶”ê°€í•˜ê¸°
    for char in ['.','!','?',';',',']:
        text = text.replace(char, ' ' + char + ' ')

    # ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ tokení™”í•˜ê¸°
    tokens = text.split()

    return tokens
```

- í…ìŠ¤íŠ¸ë¡œë¶€í„° ì–´íœ˜ì‚¬ì „ì„ êµ¬ì¶•í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•˜ì

```python
class Vocabulary:
    def __init__(self):
        # paddingê³¼ unknownì€ 0ê³¼ 1ë¡œ ë¨¼ì € ì´ˆê¸°í™”ë¥¼ í•´ì¤€ë‹¤.
        self.word2idx = {'<PAD>': 0, '<UNK>': 1}
        self.idx2word = {0: '<PAD>', 1: '<UNK>'}
        #ì²˜ìŒì— ì´ˆê¸°í™”ë¥¼ í•´ì£¼ê³  ë‚˜ë©´ ì´ˆê¸° vocab_sizeëŠ” 2ì´ë‹¤,
        self.vocab_size = 2

    def build_vocab(self, texts):
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_counts = {}
        for text in texts:
            tokens = simple_tokenize(text)
            # tokenì„ ë¶ˆëŸ¬ì™€ì„œ dictì—ì„œ tokenì˜ ë¹ˆë„ë¥¼ countí•œë‹¤.
            for token in tokens:
                if token in word_counts:
                    word_counts[token] += 1
                else:
                    word_counts[token] = 1

        # word2idx, isx2word update
        # vocab_sizeë¥¼ idxë¡œ tokenì„ encodingí•˜ëŠ” dictë¥¼ ë§Œë“ ë‹¤.
        for token in word_counts.keys():
            if token not in self.word2idx:
                self.word2idx[token] = self.vocab_size
                self.idx2word[self.vocab_size] = token
                self.vocab_size += 1

    def encode(self, text):
       
        tokens = simple_tokenize(text)
        # í† í°ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (ì—†ëŠ” ë‹¨ì–´ëŠ” <UNK> ì‚¬ìš©)
        indices = []
        for token in tokens:
            # tokenì— ëŒ€ì‘í•˜ëŠ” idxë¡œ encodingí•œë‹¤.
            if token in self.word2idx.keys():
                indices.append(self.word2idx[token])
            else:
                indices.append(self.word2idx['<UNK>'])
        return indices

    def decode(self, indices):
        # ì¸ë±ìŠ¤ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜
        words = []
        for idx in indices:
            #idx ì— ë§ëŠ” tokenìœ¼ë¡œ decodingí•œë‹¤.
            words.append(self.idx2word[idx])
        return words
```

- ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ê³ ì • í¬ê¸° ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” embedding tableì„ êµ¬í˜„ â†’ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ì€ ì •í•´ì ¸ìˆê³ , ê·¸ë³´ë‹¤ ê¸¸ì´ê°€ ì§§ë‹¤ë©´ paddingì„ í•´ì£¼ì–´ì„œ ê¸¸ì´ë¥¼ ë˜‘ê°™ì´ ë§ì¶°ì¤€ë‹¤.

```python
class WordEmbedding(nn.Module):
    def __init__(self, vocab_size, embed_dim, padding_idx=0):
        super(WordEmbedding, self).__init__()
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.padding_idx = padding_idx

        # ì„ë² ë”© í…Œì´ë¸” ìƒì„± (vocab_size x embed_dim)
        # ì´ë–„ nn.Parameterë©”ì„œë“œë¥¼ ì¨ì„œ parameterë¡œ vocab_sizeì™€ embed_dimì„ ì´ˆê¸°í™”í•´ì¤€ë‹¤.
        #xavierì´ˆê¸°í™”ë¡œ ì„ë² ë”© í…Œì´ë¸” ìƒì„±
        self.embedding_table = nn.Parameter(torch.empty(vocab_size, embed_dim))
        nn.init.xavier_uniform_(self.embedding_table)

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (í‰ê·  0, í‘œì¤€í¸ì°¨ 0.1)
        nn.init.normal_(self.embedding_table, mean=0.0, std=0.1)

        # íŒ¨ë”© í† í°ì„ ì˜ë²¡í„°ë¡œ ì„¤ì •
        self._reset_padding()

    def _reset_padding(self):

        if self.padding_idx is not None:
            with torch.no_grad():
               # padding_idxê°€ ì¡´ì¬í• ë•Œ embedding_tableì—ì„œ í•´ë‹¹ idxë¥¼ 0ìœ¼ë¡œ íŒ¨ë”©ì‹œí‚¨ë‹¤.
                self.embedding_table[self.padding_idx] = 0

    def forward(self, indices):
			# indicesí…ì„œë¥¼ ë°›ì•„ì„œ, ì„ë² ë”© ë²¡í„°ë“¤ì˜ í…ì„œë¡œ embeddingí•¨
        embeddings = self.embedding_table[indices]

        return embeddings
```

# RNN cell êµ¬í˜„

- RNN cellì˜ forward ì—°ì‚°ì„ êµ¬í˜„í•˜ì.

$h_t = \text{tanh}(h_{prev} \cdot W_{hh} + x_t \cdot W_{xh} + b_h)$

- x_t: í˜„ì¬ ì‹œì  ì…ë ¥ `(batch_size, input_size)`
- h_prev: ì´ì „ ì‹œì  ì€ë‹‰ìƒíƒœ `(batch_size, hidden_size)`
- W_hh: ì€ë‹‰-ì€ë‹‰ ê°€ì¤‘ì¹˜ `(hidden_size, hidden_size)`
- W_xh: ì…ë ¥-ì€ë‹‰ ê°€ì¤‘ì¹˜ `(input_size, hidden_size)`
- b_h: í¸í–¥ `(hidden_size,)`
- h_t: í˜„ì¬ì‹œì  ì€ë‹‰ìƒíƒœ (`batch_size, hidden_size`)

```python
def rnn_cell(x_t, h_prev, W_hh, W_xh, b_h):
    h_t = torch.tanh(h_prev @ W_hh + x_t @ W_xh + b_h)
    return h_t
```

í…ì„œë‚´ì ì—°ì‚°ì€ @ë¡œ êµ¬í˜„, ì—¬ê¸°ì„œëŠ” í…ì„œë¼ë¦¬ ì—°ì‚°í• ë•Œ, Weightì— Transposeë¥¼ í•´ì£¼ì§€ ì•Šì•„ë„, ë‚´ì ì°¨ì›ì´ ë§ì¶”ì–´ì ¸ ìˆìœ¼ë¯€ë¡œ, forward ìˆ˜ì‹ ê·¸ëŒ€ë¡œ ì—°ì‚°ì„ ì§„í–‰í•œë‹¤.

- RNN cellì„ ì´ìš©í•´ì„œ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” RNN í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•´ë³´ì.

```python
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        self.input_size = input_size

        # í•™ìŠµê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ Weightì™€ biasë¥¼ randnì´ˆê¸°í™”ë¡œ ì§„í–‰í•œë‹¤.
        self.W_hh =  nn.Parameter(torch.randn(hidden_size, hidden_size))
        self.W_xh = nn.Parameter(torch.randn(input_size, hidden_size))
        self.b_h = nn.Parameter(torch.randn(hidden_size))

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()

    def _init_weights(self):
# Xavierì´ˆê¸°í™” ì½”ë“œë¡œ êµ¬í˜„
        std = 1.0 / (self.hidden_size) ** 0.5
        for weight in self.parameters():
            weight.data.uniform_(-std, std)

    def forward(self, x, hidden=None):

        batch_size, seq_len, input_size = x.size()

        # ì´ˆê¸° ì€ë‹‰ìƒíƒœì—ì„œ Noneì¼ë•ŒëŠ” 0ë²¡í„°ë¡œ ì´ˆê¸°í™”í•œë‹¤.
        if hidden is None:
            # hiddenì„ 0ë²¡í„°ë¡œ ì´ˆê¸°í™”
            hidden = torch.zeros(batch_size, self.hidden_size)

        outputs = []

        # ì‹œí€€ìŠ¤ ê° ì‹œì ì— ëŒ€í•´ RNN ì…€ ì ìš©
        for t in range(seq_len):
            # 1. x_tëŠ” tì‹œì ì—ì„œì˜ xì´ë¯€ë¡œ seq_lenì˜ të²ˆì§¸ ê°’ë§Œì„ ê°€ì ¸ì˜¨ë‹¤.
            x_t = x[:, t, :]

            # rnn_cellí•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ tì—ì„œì˜ hidden vector êµ¬í•˜ê¸°
            h_t = rnn_cell(x_t, hidden, self.W_hh, self.W_xh, self.b_h)

            # output listì— h_të¥¼ appendí•˜ê¸°
            #ì´ë•Œ ì•„ë˜ì„œ dim=1ì„ ê¸°ì¤€ìœ¼ë¡œ catí•´ì•¼í•˜ë¯€ë¡œ unsqueezeë¥¼ ì´ìš©í•´ì„œ dim=1ì„ ì¶”ê°€í•´ì¤€ë‹¤.
            outputs.append(h_t.unsqueeze(1))
            # ê° sequence stepì—ì„œì˜ hidden update
            hidden = h_t

        # ëª¨ë“  ì‹œì ì˜ ì¶œë ¥ì„ í•˜ë‚˜ì˜ í…ì„œë¡œ ê²°í•©
        outputs = torch.cat(outputs, dim=1)
        return outputs, hidden
```

- bi-directional RNNì„ êµ¬í˜„í•´ì„œ ì‹œí€€ìŠ¤ë¥¼ ì•ë’¤ë¡œ ëª¨ë‘ ì²˜ë¦¬í•´ë³´ì.
- bi_output`:(batch_size, seq_len, hidden_size*2)`
    - output sizeë¥¼ ë§ì¶”ë ¤ë©´ forward, backwardë¡œ RNNì„ ëŒë¦° í›„ `hidden_size` dimensionìœ¼ë¡œ concatì„ í•´ì£¼ë©´ ëœë‹¤.

```python
class BiRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(BiRNN, self).__init__()
        self.hidden_size = hidden_size

        # ìˆœë°©í–¥ê³¼ ì—­ë°©í–¥ RNN -> SimpleRNNì˜ ì¸ìŠ¤í„´ìŠ¤ë¡œ ìƒì„±
        self.forward_rnn = SimpleRNN(input_size, hidden_size)
        self.backward_rnn = SimpleRNN(input_size, hidden_size)

    def forward(self, x, hidden=None):
        batch_size, seq_len, input_size = x.size()

        # ìˆœë°©í–¥ RNN
        forward_out, forward_hidden = self.forward_rnn(x, hidden)

        # ì‹œí€€ìŠ¤ ë’¤ì§‘ê¸° -> torch.flip()í•¨ìˆ˜ ì´ìš©
        x_reverse = torch.flip(x,dims=[1])
        backward_out, backward_hidden = self.backward_rnn(x_reverse, hidden)

        # ì¶œë ¥ë„ ë‹¤ì‹œ ë’¤ì§‘ê¸° -> forwardì™€ concatí•˜ê¸° ìœ„í•´ ì‹œê°„ìˆœì„œ ë‹¤ì‹œ ë§ì¶°ì£¼ê¸°
        backward_out = torch.flip(backward_out, dims = [1])

        # ìˆœë°©í–¥ê³¼ ì—­ë°©í–¥ ì¶œë ¥ ì—°ê²°
        bi_output = torch.cat((forward_out,backward_out),dim=2)

        return bi_output, (forward_hidden, backward_hidden)
```

`x_reverse = torch.flip(x,dims=[1])` : `torch.flip()`ì€ ì¸ìë¡œ ë“¤ì–´ì˜¨ í…ì„œë¥¼ dimsë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë’¤ì§‘ëŠ” í•¨ìˆ˜ì´ë‹¤. ì´ë•Œ xí…ì„œëŠ” `batch_size, seq_len, hidden_size` ë¥¼ ìˆœì„œëŒ€ë¡œ ê°€ì§€ë¯€ë¡œ, `seq_len` ë¥¼ ë’¤ì§‘ì–´ì•¼ ë¬¸ì¥ì„ ì—­ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ `dims=[1]` ì„ ì¸ìë¡œ ì¤˜ì„œ ì—­ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ í•œë‹¤.

`forward_out`ê³¼ `backward_out` ì„ concatí• ë•ŒëŠ” `bi_output` ì´ `hidden_size*2` ë¥¼ ê°€ì ¸ì•¼ í•˜ë¯€ë¡œ `dim=2` ë¥¼ ì¸ìë¡œì£¼ì–´ concatì„ í•´ì•¼ í•œë‹¤.

# LSTM ê²Œì´íŠ¸ êµ¬í˜„

![image](/assets/images/2025-10-10-14-03-04.png)

- `lstm_cell_forward()` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ì„œ LSTMì…€ì˜ í•œìŠ¤í… ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì.

```python
def lstm_cell(x_t, h_prev, c_prev, W_f, W_i, W_g, W_o, b_f, b_i, b_g, b_o):
    # [h_{t-1},x_t]ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ concatì„ í•´ì¤€ë‹¤.
    combined =  torch.cat((h_prev,x_t),dim=1)

    # ê° ê²Œì´íŠ¸ ìˆ˜ì‹ì„ ì½”ë“œë¡œ êµ¬í˜„
    f_t = torch.sigmoid(combined@W_f + b_f)
    i_t = torch.sigmoid(combined@W_i + b_i)
    g_t = torch.tanh(combined@W_g+ b_g)
    o_t = torch.sigmoid(combined@W_o + b_o)

    # ì…€ ìƒíƒœì™€ ì€ë‹‰ ìƒíƒœ ì—…ë°ì´íŠ¸
    c_t = f_t*c_prev + i_t *g_t
    h_t = o_t * torch.tanh(c_t)

    return h_t, c_

```

- `x_t` (torch.Tensor): í˜„ì¬ ì‹œì ì˜ ì…ë ¥ `(batch_size, input_size)`
- `h_prev` (torch.Tensor): ì´ì „ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ `(batch_size, hidden_size)`
- `c_prev` (torch.Tensor): ì´ì „ ì‹œì ì˜ ì…€ ìƒíƒœ `(batch_size, hidden_size)`
- `W_*, b_*` (torch.Tensor): ê° ê²Œì´íŠ¸(forget, input, gate, output)ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥

ê° ê²Œì´íŠ¸ì˜ ìˆ˜ì‹ì— ë§ê²Œ ì½”ë“œë¡œ êµ¬í˜„í•´ì£¼ì—ˆë‹¤. ì´ë•Œ ì…€ìƒíƒœì™€ ì€ë‹‰ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸ í• ë•Œ elementwise multiplicationì€ *ìœ¼ë¡œ ìˆ˜í–‰í•´ì¤€ë‹¤. ë‚´ì ì€ @ ì—°ì‚°ìë¡œ ìˆ˜í–‰í•´ì¤€ë‹¤.

- ì•ì—ì„œ êµ¬í˜„í•œ LSTM cellì„ ì‚¬ìš©í•´ì„œ SimpleLSTM í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•´ë³´ì

```python
class SimpleLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, forget_bias=1):
        super(SimpleLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.input_size = input_size

        # ëª¨ë“  Weightë¥¼ í•™ìŠµê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì •í•´ì¤€ë‹¤.
        self.W_f = nn.Parameter(torch.randn(input_size + hidden_size, hidden_size))
        self.W_i = nn.Parameter(torch.randn(input_size + hidden_size, hidden_size))
        self.W_o = nn.Parameter(torch.randn(input_size + hidden_size, hidden_size))
        self.W_g = nn.Parameter(torch.randn(input_size + hidden_size, hidden_size))

        # í¸í–¥ë„ ë§ˆì°¬ê°€ì§€ë¡œ í•™ìŠµê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì •í•´ì¤€ë‹¤.
        self.b_f = nn.Parameter(torch.ones(hidden_size) * forget_bias)
        self.b_i = nn.Parameter(torch.zeros(hidden_size))
        self.b_o = nn.Parameter(torch.zeros(hidden_size))
        self.b_g = nn.Parameter(torch.zeros(hidden_size))

        self._init_weights()

    def _init_weights(self):
        #Weightë¥¼ xavierì´ˆê¸°í™” í•´ì£¼ëŠ” í—¬í¼í•¨ìˆ˜
        std = 1.0 / np.sqrt(self.hidden_size)
        for weight in [self.W_f, self.W_i, self.W_o, self.W_g]:
            weight.data.uniform_(-std, std)

    def forward(self, x, hidden=None):
        batch_size, seq_len, input_size = x.size()

        # hiddenì´ Noneì¼ë•Œ 0ë²¡í„°ë¡œ ì´ˆê¸°í™”í•´ì¤€ë‹¤.
        if hidden == None:
            h_0 = torch.zeros(batch_size, self.hidden_size)
            c_0 = torch.zeros(batch_size, self.hidden_size)
        else:
        #hiddenì´ ì¡´ì¬í• ë•ŒëŠ” hiddenì„ h_0,c_0ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
            h_0, c_0 = hidden

        outputs = []
        h_t, c_t = h_0, c_0

        for t in range(seq_len):
            x_t = x[:,t,:]

            #lstm_cell í•¨ìˆ˜ ì´ìš©í•´ì„œ stepì§„í–‰
            h_t, c_t = lstm_cell(x_t, h_t, c_t, self.W_f, self.W_i, self.W_g, self.W_o, self.b_f, self.b_i, self.b_g, self.b_o)
            outputs.append(h_t.unsqueeze(1))

        outputs = torch.cat(outputs, dim=1)
        return outputs, (h_t, c_t)
```

- LSTMì˜ ì¥ê¸°ê¸°ì–µëŠ¥ë ¥ ë¶„ì„ ë° ì„±ëŠ¥ê°œì„ 

```python
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

def train_and_evaluate(forget_bias, seq_len=20, vocab_size=5, hidden_size=128,
                      epochs=200, batch_size=64):

    model = LSTMClassifier(vocab_size, hidden_size, forget_bias)
    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)  # í•™ìŠµë¥  ì¦ê°€
    criterion = nn.CrossEntropyLoss()

    # Learning rate scheduler ì¶”ê°€
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)

    losses = []
    accuracies = []

    for epoch in range(epochs):
        # í›ˆë ¨ ë°ì´í„° ìƒì„±
        x_train, y_train = generate_first_token_copy_data(batch_size, seq_len, vocab_size)

        # í›ˆë ¨
        model.train()
        optimizer.zero_grad()
        pred = model(x_train)
        loss = criterion(pred,y_train)
        loss.backward()

        # Gradient clipping ì¶”ê°€
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        scheduler.step()  # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§

        losses.append(loss.item())

        # í‰ê°€ (ë§¤ 5 ì—í¬í¬ë§ˆë‹¤)
        if epoch % 5 == 0:
            model.eval()
            with torch.no_grad():
                x_test, y_test = generate_first_token_copy_data(200, seq_len, vocab_size)  # ë” ë§ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°
                test_outputs = model(x_test)
                predictions = torch.argmax(test_outputs, dim=1)
                accuracy = (predictions == y_test).float().mean().item()
                accuracies.append(accuracy)

                print(f"Forget Bias {forget_bias} - Epoch {epoch}: Loss={loss.item():.4f}, Acc={accuracy:.4f}")

    return losses, accuracies
```

![image](/assets/images/2025-10-10-14-03-20.png)

<aside>
ğŸ’¡

forget_biasê°€ 0ì¼ë•ŒëŠ” ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ì§€ ì•Šì§€ë§Œ, 1ì¼ë–„ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ì´ëŸ¬í•œ ì°¨ì´ì˜ ì›ì¸ì€ ë¬´ì—‡ì¼ê¹Œ?

</aside>

 forget gateì˜ ê°’ì´ 1ë¡œ ì‹œì‘í•˜ë©´, ì´ˆê¸°ì˜ ê³¼ê±° ì •ë³´ë¥¼ ê¸°ì–µí•˜ëŠ” ìƒíƒœì—ì„œ í•™ìŠµì„ ì‹œì‘í•˜ë¯€ë¡œ ì´ˆê¸°ì˜ ì •ë³´ë¥¼ ì˜ forgetí•˜ì§€ ì•Šê³ , ì •ë³´ì†Œì‹¤ì—†ì´, ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì´ ì¼ì–´ë‚œë‹¤. í•˜ì§€ë§Œ forget_biasê°’ì´ 0ìœ¼ë¡œ ì‹œì‘í•˜ë©´, sigmoid í•¨ìˆ˜ë¥¼ ê±°ì³¤ì„ë•Œ 0.5ì— ì´ˆê¸°ì¶œë ¥ê°’ì´ ê°€ê¹Œì›Œì§€ê³ , ì´ëŠ” ê³¼ê±° ì •ë³´ì˜ ì ˆë°˜ì„ ì²˜ìŒë¶€í„° ë²„ë¦¬ê³  ì‹œì‘í•˜ëŠ”ê²ƒê³¼ ê°™ì•„ì„œ, Gradient Vanishingë¬¸ì œê°€ ë°œìƒí•œë‹¤.